{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Why Singularity/Apptainer?","text":"<ul> <li>A secure, single-file based container format SIF\u2122 (Singularity Image Format) is a single executable file based container image, cryptographically signed, auditable, secure, and easy to move using existing data mobility paradigms.</li> <li>Support for data-intensive workloads The elegance of Singularity's architecture bridges the gap between HPC and AI, deep learning/machine learning, and predictive analytics.</li> <li>Extreme mobility Use standard file and object copy tools to transport, share, or distribute a Singularity container. Any endpoint with Singularity installed can run the container.</li> <li>Compatibility Designed to support complex architectures and workflows, Singularity is easily adaptable to almost any environment.</li> <li>More ...</li> <li>More from the \"User documentation\" ...</li> </ul>"},{"location":"#what-is-singularityapptainer","title":"What is Singularity/Apptainer.","text":"<p>Singularity is not the only OS level virtualization implementation around. One of the main uses of Singularity is to bring containers and reproducibility to scientific computing and the high-performance computing (HPC) world<sup>1</sup>.</p> <p>More on Wikipedia</p>"},{"location":"#what-is-singularityapptainer-an-alternative-view","title":"What is Singularity/Apptainer - an alternative view.","text":"<p>Singularity runs in the user space i.e. which allows you to run Singularity containers in systems where you have only user rights - common situation on public and government computer resources.</p> <p>Since your home folder gets automatically mounted/exposed to your virtual environment you can look at it as an alternative way to expose your data to different complete setups with pre-installed and configured software.</p>"},{"location":"#purpose","title":"Purpose","text":"<p>This workshop material aims to demonstrate and exercise some commonly used features by simple interactive tutorials. Thus, this is not complete manual or documentation for Singularity.</p> <p>The \"Singularity user documentation\"  and \"Apptainer user documentation\" are excellent reference sources with basic examples in well-ordered fashion and always up to date.</p> <ol> <li> <p>Kurtzer, Gregory M; Sochat, Vanessa; Bauer, Michael W (2017). \"Singularity: Scientific containers for mobility of compute\". PLOS ONE. 12 (5): e0177459. Bibcode:2017PLoSO..1277459K. doi:10.1371/journal.pone.0177459. PMC 5426675. PMID 28494014 \u21a9</p> </li> </ol>"},{"location":"PyTorch_NVIDIA/","title":"NVIDIA Deep Learning Frameworks","text":"<p>Here is how easy one can use an NVIDIA environment for deep learning with all the following tools preset.</p> <p></p> <p>First, Let's pull the container (6.5GB).</p> <pre><code>singularity pull docker://nvcr.io/nvidia/pytorch:22.03-py3\n</code></pre> <p>When done, get an interactive shell.</p> <pre><code>singularity shell --nv ~/external_1TB/tmp/pytorch_22.03-py3.sif\n\nSingularity&gt; python3\nPython 3.8.12 | packaged by conda-forge | (default, Jan 30 2022, 23:42:07) \n[GCC 9.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n&gt;&gt;&gt; import torch\n# Check torch version\n&gt;&gt;&gt; print(torch.__version__) \n1.12.0a0+2c916ef\n\n# Check if CUDA is available\n&gt;&gt;&gt; print(torch.cuda.is_available()) \nTrue\n\n# Check which GPU architectures are supported\n&gt;&gt;&gt; print(torch.cuda.get_arch_list()) \n['sm_52', 'sm_60', 'sm_61', 'sm_70', 'sm_75', 'sm_80', 'sm_86', 'compute_86']\n\n# test torch\n&gt;&gt;&gt; torch.zeros(1).to('cuda')\ntensor([0.], device='cuda:0')\n</code></pre> <p>From the container shell, check what else is available...</p> <pre><code>Singularity&gt; nvcc -V\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2022 NVIDIA Corporation\nBuilt on Thu_Feb_10_18:23:41_PST_2022\nCuda compilation tools, release 11.6, V11.6.112\nBuild cuda_11.6.r11.6/compiler.30978841_0\n\n# Check what conda packages are already there\nSingularity&gt; conda list -v\n\n# Start a jupyter-lab (keep in mind the hostname)\nSingularity&gt; jupyter-lab\n...\n[I 13:35:46.270 LabApp] [jupyter_nbextensions_configurator] enabled 0.4.1\n[I 13:35:46.611 LabApp] jupyter_tensorboard extension loaded.\n[I 13:35:46.615 LabApp] JupyterLab extension loaded from /opt/conda/lib/python3.8/site-packages/jupyterlab\n[I 13:35:46.615 LabApp] JupyterLab application directory is /opt/conda/share/jupyter/lab\n[I 13:35:46.616 LabApp] [Jupytext Server Extension] NotebookApp.contents_manager_class is (a subclass of) jupytext.TextFileContentsManager already - OK\n[I 13:35:46.616 LabApp] Serving notebooks from local directory: /home/pmitev\n[I 13:35:46.616 LabApp] Jupyter Notebook 6.4.8 is running at:\n[I 13:35:46.616 LabApp] http://hostname:8888/?token=d6e865a937e527ff5bbccfb3f150480b76566f47eb3808b1\n[I 13:35:46.616 LabApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n...\n</code></pre> <p>You can use this container to add more packages. <pre><code>Bootstrap: docker\nFrom: nvcr.io/nvidia/pytorch:22.03-py3\n...\n</code></pre> Just keep in mind that \"upgrading\" the build-in <code>torch</code> package might install a package that is compatible with less GPU architectures and it might not work anymore on your hardware.</p> <pre><code>Singularity&gt; python3 -c \"import torch; print(torch.__version__); print(torch.cuda.is_available()); print(torch.cuda.get_arch_list()); torch.zeros(1).to('cuda')\"\n\n1.10.0+cu102\nTrue\n['sm_37', 'sm_50', 'sm_60', 'sm_70']\nNVIDIA A100-PCIE-40GB with CUDA capability sm_80 is not compatible with the current PyTorch installation.\nThe current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n</code></pre>"},{"location":"about/","title":"About","text":"<p>This web page is a supplementary material to the NAISS \"Basic Singularity/Apptainer\" workshop.</p>"},{"location":"about/#singularity-user-guildes-at-snic-centers","title":"Singularity user guildes at SNIC centers","text":"<ul> <li>UPPMAX</li> <li>NSC</li> <li>HPC2N</li> <li>PDC</li> <li>C3SE</li> </ul>"},{"location":"about/#course-feedback","title":"Course feedback","text":"<ul> <li>2024.10.16 |  2024.02.23</li> <li>2023.10.06 |  2023.03.10</li> <li>2022.09.28 |  2022.04.20</li> <li>2021.11.17 |  2021.05.03</li> </ul> <p>* can not be anonymised or violates GDPR</p> <p>Acknowledgment</p> <p>This page is supported by</p> <ul> <li>NAISS (2024 - )</li> <li>UPPMAX (2021 - )</li> <li>SNIC (2021 - 2022)</li> </ul>"},{"location":"about/#contacts","title":"Contacts:","text":"<ul> <li>Pavlin Mitev - UPPMAX</li> <li>Pedro Ojeda May - HPC2N</li> </ul>"},{"location":"build_container/","title":"Building your first container","text":"<p>In this simple example we will build Singularity container that will run the following programs <code>fortune | cowsay | lolcat</code> by installing all necessary libraries and packages within Ubuntu 16.04 Linux distribution setup.</p>"},{"location":"build_container/#simple-singularity-definition-file","title":"Simple Singularity definition file","text":"<p>lolcow.def</p> <pre><code>BootStrap: docker\nFrom: ubuntu:16.04\n\n%post\n  apt-get -y update\n  apt-get -y install fortune cowsay lolcat\n\n%environment\n  export LC_ALL=C\n  export PATH=/usr/games:$PATH\n\n%runscript\n  fortune | cowsay | lolcat\n</code></pre>"},{"location":"build_container/#building-the-container","title":"Building the container","text":"<pre><code>$ sudo singularity build lolcow.sif lolcow.def\n\nStarting build...\nGetting image source signatures\nCopying blob 4007a89234b4 done  \nCopying blob 5dfa26c6b9c9 done  \nCopying blob 0ba7bf18aa40 done  \nCopying blob 4c6ec688ebe3 done  \nCopying config 24336f603e done  \nWriting manifest to image destination\nStoring signatures\n\n...\n\nINFO:    Adding environment to container\nINFO:    Adding runscript\nINFO:    Creating SIF file...\nINFO:    Build complete: lolcow.sif\n</code></pre>"},{"location":"build_container/#run-the-singularity-container","title":"Run the Singularity container","text":"<p><pre><code>$ ./lolcow.sif \n_________________________________________\n/ You will stop at nothing to reach your  \\\n| objective, but only because your brakes |\n\\ are defective.                          /\n -----------------------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre> -=&gt;&gt;&gt; Done &lt;&lt;&lt;=-</p> <p> asciinema</p>"},{"location":"build_container/#syntax-of-the-definition-file","title":"Syntax of the definition file","text":"<p>lolcow.def</p> <pre><code>BootStrap: docker\nFrom: ubuntu:16.04\n\n%post\n  apt-get -y update\n  apt-get -y install fortune cowsay lolcat\n\n%environment\n  export LC_ALL=C\n  export PATH=/usr/games:$PATH\n\n%runscript\n  fortune | cowsay | lolcat\n</code></pre>"},{"location":"build_container/#header-bootsrap-agents-online-documentation","title":"header: Bootsrap agents - online documentation","text":"<ul> <li><code>library</code> - images hosted on the Container Library</li> <li><code>docker</code> - images hosted on Docker Hub</li> <li><code>shub</code> - images hosted on Singularity Hub</li> <li>...</li> <li>Other: <code>localimage</code>, <code>yum</code>, <code>debootstrap</code>, <code>oci</code>, <code>oci-archive</code>, <code>docker-daemon</code>, <code>docker-archive</code>, <code>arch</code>, <code>busybox</code>, <code>zypper</code></li> </ul>"},{"location":"build_container/#header-from","title":"header: From","text":"<p>Depending on the value assigned to <code>Bootstrap</code>, other keywords may also be valid in the header. For example, when using the <code>library</code> bootstrap agent, the <code>From</code> keyword becomes valid.</p>"},{"location":"build_container/#post","title":"%post","text":"<p>This section is where you can download files from the Internet, install new software and libraries, write configuration files, create new directories, etc.</p>"},{"location":"build_container/#environment","title":"%environment","text":"<p>The <code>%environment</code> section allows you to define environment variables that will be set at runtime.</p>"},{"location":"build_container/#runscript","title":"%runscript","text":"<p>The contents of the <code>%runscript</code> section are written to a file within the container that is executed when the container image is run (either via the <code>singularity run</code> command or by executing the container directly as a command)</p>"},{"location":"build_container/#brief-summary-with-examples-online-documentation","title":"Brief summary with examples - online documentation","text":""},{"location":"build_container/#all-sections-online-documentation","title":"All sections - online documentation","text":""},{"location":"build_container/#installing-software-from-a-local-package","title":"Installing software from a local package","text":"<p>Sometimes, you cannot download a package directly or the software needs signing licenses. In this case you need to push in the locally downloaded file during the build process. You can get the latest version of the file bellow from here: https://jp-minerals.org/vesta/en/download.html (download the the linux <code>VESTA-gtk3.tar.bz2</code> version).</p> <p>vesta.def</p> <pre><code>Bootstrap:  docker\nFrom: ubuntu:20.04\n\n%files\n  VESTA-gtk3.tar.bz2 / \n\n%post\n  export DEBIAN_FRONTEND=noninteractive\n\n  apt-get update &amp;&amp; apt-get -y dist-upgrade &amp;&amp; \\\n  apt-get install -y libxmu6 libxss1 libxft2 libquadmath0 libpng16-16 bzip2 libgl1-mesa-glx \\\n  libglu1-mesa libglib2.0-0 libgtk-3-0 libgtk-3-dev libgomp1 &amp;&amp; \\\n  apt-get clean\n\n  # Install/unpack the precompiled software\n  tar -C /usr/local -xvf /VESTA-gtk3.tar.bz2 &amp;&amp; rm /VESTA-gtk3.tar.bz2   \n\n%runscript\n  /usr/local/VESTA-gtk3/VESTA \"$@\"\n</code></pre> <p>Note the <code>%files</code> section. The line bellow will copy <code>VESTA-gtk3.tar.bz2</code> from the current directory to the root folder  <code>\\</code> in the Singularity container. Also, you need to figure out yourself all required libraries and dependencies and install them.</p>"},{"location":"collections/","title":"List with links to Singularity recipes online","text":"<ul> <li>https://github.com/sylabs/examples - examples from Sylabs itself</li> <li>https://github.com/accre/singularity - examples that use singularity containers to run arbitrary operating systems on the ACCRE cluster.</li> <li>https://github.com/hpcng/singularity/tree/master/examples - variety of examples</li> </ul>"},{"location":"docker2singularity/","title":"Converting Docker image to Singularity container","text":""},{"location":"docker2singularity/#1-run-directly-from-dockerhub","title":"1. run \"directly\" from DockerHub","text":"<p>This is most common situation, already mentioned. You can directly run the container. The content of the Docker image will be pulled and cached localy. <pre><code>$ singularity run docker://godlovedc/lolcow\n</code></pre></p>"},{"location":"docker2singularity/#2-pullbuild-from-dockerhub","title":"2. pull/build from DockerHub","text":"<p>With <code>pull</code> the singularity name gets automatically assigned, in this example to \"lolcow_latest.sif\". Note, this operation does not need <code>sudo</code>.</p> <pre><code>$ singularity pull docker://godlovedc/lolcow\n\nINFO:    Converting OCI blobs to SIF format\nINFO:    Starting build...\n...\nINFO:    Creating SIF file...\n\n# name gets automatically assigned to \"lolcow_latest.sif\"\n</code></pre> <pre><code>$ singularity build lolcow.sif docker://godlovedc/lolcow\n\nStarting build...\nGetting image source signatures\nCopying blob 9fb6c798fa41 done  \nCopying blob 3b61febd4aef done\n...\nINFO:    Creating SIF file...\nINFO:    Build complete: lolcow.sif\n</code></pre>"},{"location":"docker2singularity/#3-building-from-docker-image-on-your-computer","title":"3. Building from docker image on your computer","text":"<p>The examples bellow assume you have permission to run docker.</p> <pre><code># Build the docker image\n$ docker build -t local/my_container .\n\n# build from local docker repository\n$ sudo singularity build my_container.sif docker-daemon://local/my_container\n</code></pre>"},{"location":"docker2singularity/#4-using-docker-image-saved-in-a-tar-file","title":"4. Using Docker image saved in a .tar file","text":"<pre><code># Save the docker container from your computer to a .tar file\n$ docker save image_id -o local.tar\n\n# Build from a docker-archive\n#$ singularity build local_tar.sif docker-archive://local.tar\n$ singularity build local_tar.sif local.tar\n</code></pre> <p>Related documentation online...</p>"},{"location":"exercise_00/","title":"Simple build to exercise","text":"<p>Let's start with something easy and fast to build - Install the figlet app in a container.</p> <pre><code>$ figlet UPPMAX\n _   _ ____  ____  __  __    _    __  __\n| | | |  _ \\|  _ \\|  \\/  |  / \\   \\ \\/ /\n| | | | |_) | |_) | |\\/| | / _ \\   \\  / \n| |_| |  __/|  __/| |  | |/ ___ \\  /  \\ \n \\___/|_|   |_|   |_|  |_/_/   \\_\\/_/\\_\\\n\n $ figlet -f slant UPPMAX\n   __  ______  ____  __  ______   _  __\n  / / / / __ \\/ __ \\/  |/  /   | | |/ /\n / / / / /_/ / /_/ / /|_/ / /| | |   / \n/ /_/ / ____/ ____/ /  / / ___ |/   |  \n\\____/_/   /_/   /_/  /_/_/  |_/_/|_|\n</code></pre> <p>Let's use Ubuntu from Docker to install the package.</p> figlet.def <pre><code>Bootstrap: docker\nFrom: ubuntu:20.04\n</code></pre> <ul> <li>It is not necessary right now, but it is good to inform the package manager that we are not in a interactive session by <code>export DEBIAN_FRONTEND=noninteractive</code></li> <li>We need only to install the figlet package by apt-get. Do not forget to <code>apt-get update</code> first, since the package index is empty.</li> <li><code>apt-get install -y figlet</code> - use <code>-y</code> to avoid the confirmation when installing packages</li> <li>In what section we add these commands?</li> <li>Let's clean a bit with <code>apt-get clean</code></li> </ul> figlet.def <pre><code>Bootstrap: docker\nFrom: ubuntu:20.04\n\n%post\n  export DEBIAN_FRONTEND=noninteractive\n\n  apt-get update\n  apt-get install -y figlet\n  apt-get clean\n</code></pre> <ul> <li>Let's define what to run when we run the container itself.</li> </ul> figlet.def <pre><code>Bootstrap: docker\nFrom: ubuntu:20.04\n\n%post\n  export DEBIAN_FRONTEND=noninteractive\n\n  apt-get update\n  apt-get install -y figlet\n  apt-get clean\n\n%runscript\n  figlet \"$@\"\n</code></pre> <ul> <li>Build the recipe</li> </ul> build <pre><code>$ sudo singularity build figlet.sif figlet.def\n</code></pre>"},{"location":"exercise_02/","title":"Build, sign, and upload Singularity container to Sylabs container library.","text":"<p>Online material</p>"},{"location":"exercise_02/#1-make-an-account","title":"1. Make an account","text":"<ol> <li>Go to: https://cloud.sylabs.io/library.</li> <li>Click \"Sign in to Sylabs\" (top right corner).</li> <li>Select your method to sign in, with Google, GitHub, GitLab, or Microsoft.</li> <li>Type your passwords, and that's it!</li> </ol>"},{"location":"exercise_02/#2-create-an-access-token-and-login-link","title":"2. Create an access token and login: link","text":""},{"location":"exercise_02/#3-pushing-container","title":"3. Pushing container","text":"<pre><code>$ singularity push my-container.sif library://your-name/project-dir/my-container:latest\n</code></pre> <p>Info</p> <ul> <li>Use lower case for <code>project-dir</code> to avoid strange problems.</li> <li>You might need to use <code>-U</code> or <code>--allow-unsigned</code> to push the container.</li> </ul>"},{"location":"exercise_02/#4-sign-your-container-optional-link","title":"4. Sign your container (optional): link","text":""},{"location":"fakeroot/","title":"Building without elevated privileges with <code>--fakeroot</code>","text":"<p>Online manual</p> <p>The fakeroot feature (commonly referred as rootless mode) allows an unprivileged user to run a container as a \"fake root\" user by leveraging user namespace UID/GID mapping.</p> <p>A \"fake root\" user has almost the same administrative rights as root but only inside the container and the requested namespaces, which means that this user:</p> <ul> <li>can set different user/group ownership for files or directories they own</li> <li>can change user/group identity with su/sudo commands</li> <li>has full privileges inside the requested namespaces (network, ipc, uts)</li> </ul> <p>Note</p> <p>Many computer centers, do not allow the use of \"fake root\" and attempt to build  trigger the following error: <pre><code>$ singularity build --fakeroot lolcow.sif lolcow.def \nFATAL:   could not use fakeroot: no mapping entry found in /etc/subuid for user\n</code></pre></p> <p>UPDATE 2022.10.19: Alvis and Rackham support building Singularity containers with <code>apptainer / singularity</code> UPDATE 2023.03.08: Kebnekaise supports building containers with <code>apptainer</code> fakeroot option (Apptainer available on the command-line) <pre><code>$ apptainer build lolcow.sif lolcow.def \nINFO:    Detected Singularity user configuration directory\nINFO:    User not listed in /etc/subuid, trying root-mapped namespace\nINFO:    The %post section will be run under fakeroot\nINFO:    Starting build...\n...\nINFO:    Adding environment to container\nINFO:    Adding runscript\nINFO:    Creating SIF file...\nINFO:    Build complete: lolcow.sif\n</code></pre> asciinema</p>"},{"location":"fakeroot/#handy-environmental-variables-for-use-on-hpc-clusters","title":"Handy environmental variables for use on HPC clusters","text":"<p>Environmental variables that will help you to redirect potentially large folders to alternative location - keep in mind that your <code>$HOME</code> folder is relatively small in size.</p> <pre><code>export PROJECT=project_folder\n\nexport SINGULARITY_CACHEDIR=/proj/${PROJECT}/nobackup/SINGULARITY_CACHEDIR\nexport SINGULARITY_TMPDIR=/proj/${PROJECT}/nobackup/SINGULARITY_TMPDIR\n\nexport APPTAINER_CACHEDIR=/proj/${PROJECT}/nobackup/SINGULARITY_CACHEDIR\nexport APPTAINER_TMPDIR=/proj/${PROJECT}/nobackup/SINGULARITY_TMPDIR\n\nmkdir -p $APPTAINER_CACHEDIR $APPTAINER_TMPDIR\n</code></pre>"},{"location":"fakeroot/#documentation-about-singularity-apptainer-on-different-hpc-centers","title":"Documentation about  Singularity / Apptainer on different HPC centers:","text":"<ul> <li>C3SE<ul> <li>Alvis - https://www.c3se.chalmers.se/documentation/applications/containers/</li> </ul> </li> <li>PDC<ul> <li>Dardel - https://www.pdc.kth.se/software/software/singularity/index_general.html</li> </ul> </li> <li>HPC2N<ul> <li>https://www.hpc2n.umu.se/resources/software/singularity</li> </ul> </li> <li>NSC<ul> <li>https://www.nsc.liu.se/support/singularity/</li> </ul> </li> <li>Lunarc<ul> <li>https://lunarc-documentation.readthedocs.io/en/latest/guides/containers/apptainer/</li> </ul> </li> </ul>"},{"location":"gitpod/","title":"Building Apptainer containers interactivelly on Gitpod.","text":"<p>There is an experimental build with graphical interface and Apptainer running in Gitpod. The free tier allows users to run about 50 hours on the standard configuration (4 cores, 8GB RAM, ~30GB storage).  </p> <p></p> <p>To start the session, click on the button above, follow the instructions to link your GitHub/GitLab etc. account, wait for the build and start the session.</p> <ol> <li>Select the default options for the \"New Workspace\" </li> <li>Click on \"Continue\" and wait for the build - this might take some 10-15 minutes.  </li> <li>The Workspace will open VScode interface with a preview of the graphical interface (2). You can use the terminal (1) to build in text or click in the top right corner to open the graphical interface in a separate tab in the browser.  If you close the Workspace by accident,    again on the button above, go to https://gitpod.io/workspaces and open the already build workspace. If you have closed by accident the graphical interface, from console (1) run  <pre><code>gp preview --external $(gp url 6080)\n</code></pre> Copy/paste from your computer to the graphical interface is a bit cumbersome  so, perhaps it is better to open Chrome from the \"Applications/Internet/Google Chrome\" menu and copy paste the material from there.</li> <li>To build a recipe run (you can use <code>singularity</code> as well - it will still run <code>apptainer</code>) <pre><code>sudo apptainer build lolcow.sif lolcow.def\n</code></pre></li> <li>Run as usually. </li> </ol> <p>The Gitpod workspace runs Ubuntu 22.04 and you can:</p> <ul> <li>install packages with <code>sudo apt-get ...</code></li> <li>install python modules with <code>python3 -m pip install ...</code> via <code>pyenv</code>. To get the newly installed modules and tools in the path you need to refresh with <code>pyenv rehash</code> after installation.</li> </ul> <p>Files created in repository folder will be kept when the workspace is restarted - any other files will be lost.</p>"},{"location":"goods_bads/","title":"Common good or bad practices for building Singularity containers","text":"<p>Disclaimer: these might not be the best solutions at all.</p>"},{"location":"goods_bads/#where-to-compile-and-install-source-codes","title":"Where to compile and install source codes.","text":"<p>There are some, not so easy to see, complications. <code>/tmp</code> looks like good choice... The problem is that /tmp is mounted automatically even during the build process. This means that you will collide with leftovers from previous builds which might lead to rather unexpected results. We will use this problematic behavior in the next section for our advantage.</p> <p><code>$HOME</code> points to <code>/root</code> during build, and it is also mounted at build time. Really bad place to compile!</p> <p>So... Where is a good place to compile and install?</p> <p>Here is an example scenario</p> <pre><code>%environment\n  export PATH=/opt/tool-name/bin:${PATH}\n\n%post\n  mkdir -p /installs &amp;&amp; cd /installs\n  git clone repository-link &amp;&amp; cd tool-name\n  ./configure --prefix=/opt/tool-name\n  make &amp;&amp; make install\n\n...\n\n  # Clean the installation folder (unless you want to keep it)\n  rm -rf /installs\n</code></pre> <ul> <li>dedicate a folder in the container's file structure</li> <li>fetch your installation files there (look how this might be improved for large files downloaded with <code>wget</code>)</li> <li>install in /opt and adjust the <code>$PATH</code>   or just allow the tool to mix with the system files.</li> </ul>"},{"location":"goods_bads/#conda","title":"Conda","text":"<p>Conda causes some unexpected problems. During the build and and the commands in <code>%runscrupt</code> sections are run with <code>/bin/sh</code> which fails upon <code>source /full_path_to/conda.sh</code> which in turn fails <code>conda activate my_environment</code>. Her are two examples how to deal with the situation.</p> <p>docker://continuumio/miniconda3 container</p> <pre><code>Bootstrap: docker\nFrom: continuumio/miniconda3\n\n%environment\n  export LC_ALL=C\n\n%post\n  export LC_ALL=C\n\n  conda config --add channels defaults\n  conda config --add channels conda-forge\n  conda config --add channels bioconda\n  conda config --add channels ursky\n  conda create --name metawrap-env --channel ursky metawrap-mg=1.3.2 tbb=2020.2\n\n  conda clean --all --yes\n\n%runscript\n  params=$@\n/bin/bash &lt;&lt;EOF\n  source /opt/conda/etc/profile.d/conda.sh\n  conda activate metawrap-env\n  metawrap $params\nEOF\n</code></pre> <p>Tip: for <code>mamba</code> one can start from <code>From: condaforge/mambaforge</code></p> Ubuntu + conda <pre><code>Bootstrap: docker\nFrom: ubuntu:20.04\n\n%labels\n  Author pmitev@gmail.com\n\n%environment\n  export LC_ALL=C\n  export CONDA_ENVS_PATH=/opt/conda_envs\n  export PATH=/opt/metaWRAP/bin:$PATH\n\n%post\n  export DEBIAN_FRONTEND=noninteractive\n  export LC_ALL=C\n  export CONDA_ENVS_PATH=/opt/conda_envs &amp;&amp;  mkdir -p ${CONDA_ENVS_PATH}\n\n  apt-get update &amp;&amp; apt-get -y install  wget git\n\n  mkdir /installs &amp;&amp; cd /installs\n\n  # Conda installation     ==============================================\n  mconda=\"Miniconda3-py38_4.9.2-Linux-x86_64.sh\"\n  wget https://repo.anaconda.com/miniconda/${mconda} &amp;&amp; \\\n  chmod +x ${mconda} &amp;&amp; \\\n  ./${mconda} -b -p /opt/miniconda3 &amp;&amp; \\\n  ln -s /opt/miniconda3/bin/conda /usr/bin/conda\n\n  # metaWRAP dependencies installation     ==============================\n/bin/bash &lt;&lt;EOF\n  source /opt/miniconda3/etc/profile.d/conda.sh\n\n  conda create -y -n metawrap-env python=2.7\n  conda activate metawrap-env\n\n  conda config --add channels defaults\n  conda config --add channels conda-forge\n  conda config --add channels bioconda\n  conda config --add channels ursky\n\n  conda install --only-deps -c ursky metawrap-mg\n\n  conda clean --all --yes\nEOF\n\n  # metaWRAP from github     ============================================\n  cd /opt\n  git clone https://github.com/bxlab/metaWRAP.git\n\n  cd / &amp;&amp; rm -r /installs\n\n%runscript\n  params=$@\n/bin/bash &lt;&lt;EOF\n  export PATH=/opt/metaWRAP/bin:$PATH\n  source /opt/miniconda3/etc/profile.d/conda.sh\n  conda activate metawrap-env\n  metawrap $params\nEOF\n</code></pre>"},{"location":"goods_bads/#pip","title":"pip","text":"<p>Install only the minimum python (<code>python3-dev</code>) from the distribution package manager and the equivalent for <code>build-essential</code>. The rest should be perhaps better done by <code>pip</code>. Some libraries might still be needed.</p>"},{"location":"goods_bads/#downloading-packages-and-files-multiple-times","title":"Downloading packages and files multiple times.","text":""},{"location":"goods_bads/#package-installation-apt-yum-etc","title":"Package installation - apt, yum, etc...","text":"<p>Even if you use <code>--sandbox</code> you might find that some commands do not behave the same way as when executed by the common routines <code>sudo build...</code>. Some of these problems are related to the shell interpreter which might be <code>sh</code> or <code>bash</code>...</p> <p>Warning</p> <p>This nice file fetching trick will work interactively when you test but it will fail during the build <pre><code>wget -P bcftools/plugins https://raw.githubusercontent.com/freeseek/gtc2vcf/master/{gtc2vcf.{c,h},affy2vcf.c}\n</code></pre> It needs to be tricked a bit. <pre><code>/bin/bash -c 'wget -P bcftools/plugins https://raw.githubusercontent.com/freeseek/gtc2vcf/master/{gtc2vcf.{c,h},affy2vcf.c}'\n</code></pre></p> <p>Now, if you find yourself repeatedly rebuilding your definition file... and you find that every time you need to re-download packages from the repositories... Some hosting services might slow you down or even block you upon repetitive downloads...</p> <pre><code>%post\n  mkdir -p /tmp/apt\n  echo \"Dir::Cache \"/tmp/apt\";\" &gt; /etc/apt/apt.conf.d/singularity-cache.conf\n\n  apt-get update &amp;&amp; \\\n  apt-get --no-install-recommends -y install  wget unzip git bwa samtools\n  # the usual clean up\n  rm -rf /var/lib/apt/lists/*\n\n  ...\n\n  # remove the /tmp/apt caching configuration\n  rm /etc/apt/apt.conf.d/singularity-cache.conf\n</code></pre> <p>Note</p> <ul> <li>Remember to remove these lines in the final recipe.</li> <li>note the <code>--no-install-recommends</code> which can save on installing unnecessary packages. It is rather popular option.</li> </ul>"},{"location":"goods_bads/#downloading-large-files","title":"Downloading large files","text":"<p>The example bellow is from the installation instructions for https://github.com/freeseek/gtc2vcf.</p> <p>Here is the original code, which downloads the 871MB file and extracts it on the fly. Then some indexing is applied. <pre><code>wget -O- ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/human_g1k_v37.fasta.gz | \\\n  gzip -d &gt; $HOME/GRCh37/human_g1k_v37.fasta\nsamtools faidx $HOME/GRCh37/human_g1k_v37.fasta\nbwa index $HOME/GRCh37/human_g1k_v37.fasta\n</code></pre></p> <p>The file is rather large for multiple downloads... we could rewrite a bit the lines like this and keep the original file during builds.</p> <pre><code>%post\n\n  export TMPD=/tmp/downloads\n  mkdir -p $TMPD\n\n  # Install the GRCh37 human genome reference =======================================\n  mkdir -p /data/GRCh37 &amp;&amp; cd /data/GRCh37\n\n  wget -P $TMPD -c  ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/human_g1k_v37.fasta.gz\n  gunzip -c $TMPD/human_g1k_v37.fasta.gz &gt; human_g1k_v37.fasta || true\n\n  samtools faidx /data/GRCh37/human_g1k_v37.fasta\n  bwa index /data/GRCh37/human_g1k_v37.fasta || true\n</code></pre> <p>Note</p> <p><code>gunzip</code> is returning non-zero exit code which signals an error and the Singularity build will stop. The not so nice solution is to apply the <code>|| true</code> \"trick\" to ignore the error. Similar for the <code>bwa</code> tool.</p> <p>Warning</p> <p>The <code>samtools</code> and <code>bwa</code> are computationally intensive, memory demanding, and time demanding. This will conflict with some of the limitations of the free online building services. You might consider doing this outside the container and only copy the files (the uncompressed result is even larger) or better - as in the original instructions they will be installed in the user's <code>$HOME</code> directory.</p> <p>Have look for alternative advanced ideas - Image Mounts</p>"},{"location":"goods_bads/#installing-r-and-libraries","title":"Installing R and libraries","text":"<p>Warning</p> <p>If you are using <code>vagrand</code> to run Singularity, keep in mind that installing R libraries often might need more than 4GB memory, which needs increasing the memory of the instance. Inspect the build log for failures... singularity does not catch them and continues building...</p> <p>Here are some tips (try them but they might be autdated).</p> <pre><code>Bootstrap: docker\nFrom: ubuntu:20.04\n\n%post\n\n  # R-CRAN\n  apt-get -y  install dirmngr gnupg apt-transport-https ca-certificates software-properties-common\n  apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9\n  add-apt-repository 'deb https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/'\n  apt-get update &amp;&amp; apt-get -y  install r-base\n\n  # Add a default CRAN mirror\n  echo \"options(repos = c(CRAN = 'https://cran.rstudio.com/'), download.file.method = 'libcurl')\" &gt;&gt; /usr/lib/R/etc/Rprofile.site\n\n  # Rstudio\n  wget -P /tmp/ -c https://download1.rstudio.org/desktop/bionic/amd64/rstudio-2021.09.1-372-amd64.deb\n  apt-get -y install /tmp/rstudio-2021.09.1-372-amd64.deb\n\n  # Fix R package libpaths (helps RStudio Server find the right directories)\n  mkdir -p /usr/lib64/R/etc\n  echo \"R_LIBS_USER='/usr/lib64/R/library'\" &gt;&gt; /usr/lib64/R/etc/Renviron\n\n  # Install reticulate \n  Rscript -e 'install.packages(\"reticulate\")'\n\n  # Perhaps miniconda via \n  Rscript -e 'reticulate::install_miniconda()'\n</code></pre> <p>Here you can find more detailed instructions related to different ideas related to R: link.</p>"},{"location":"goods_bads/#compiling-code","title":"Compiling code...","text":"<p>... and cleaning the development tools and libraries to slim-down the container.</p> <p>Warning</p> <p>Removing the build dependencies, might remove some necessary libraries - you need to install them back if necessary.</p> <pre><code>...\n\n# install packages needed for compiling \ndeps=\"wget git make cmake gcc g++ gfortran\"\napt-get install -y --no-install-recommends $deps\n\n# install packages needed for OpenGL\napt-get install -y --no-install-recommends mesa-utils ...\n\n# compile some code here\n\n# remove build dependencies\napt-get purge -y --auto-remove $deps\n\n...\n</code></pre>"},{"location":"goods_bads/#kernel-dependencies","title":"Kernel dependencies","text":"<p>Nowadays, <code>glibc</code>, and probably other libraries, occasionally take advantage of new kernel syscalls. Singulariy images run with the host machine's kernel. Debian 9 has an old enough <code>glibc</code> to not have many features that would only work on newer machines, and the other packages are new enough to compile all of these dependencies. Consider <code>FROM debian:9</code> or <code>FROM ubuntu:18.04</code> to address such problems.</p>"},{"location":"indirect-call/","title":"Indirect executable calls","text":"<p>The common approach for Singularity is to have single entry point defined by <code>%runscript</code> or by <code>%app</code>. This is not so convenient for daily use... So, here is a minimal example on how to implement a well known trick to use single executable for multiple commands (see for example the BusyBox project).</p> <pre><code>Bootstrap: docker\nFrom: ubuntu:20.04\n\n%environment\n  export LC_ALL=C\n\n%post\n  export LC_ALL=C\n  export DEBIAN_FRONTEND=noninteractive\n\n  mkdir -p /tmp/apt\n  echo \"Dir::Cache /tmp/apt;\" &gt; /etc/apt/apt.conf.d/singularity-cache.conf\n\n  apt-get update &amp;&amp; \\\n  apt-get --no-install-recommends -y install  wget unzip git bwa samtools bcftools bowtie\n\n%runscript\n  if command -v $SINGULARITY_NAME &gt; /dev/null 2&gt; /dev/null; then\n    exec $SINGULARITY_NAME \"$@\"\n  else\n    echo \"# ERROR !!! Command $SINGULARITY_NAME not found in the container\"\n  fi\n</code></pre> <p>Let's build the recipe and make soft links to the executables in the image: <pre><code>sudo singularity build samtools.sif Singularity.samtools\n\n# make bin folder\nmkdir -p bin\n\n# Extraxt the executable names from the packages of interest\nSIMG=samtools.sif\nbins=$(singularity exec ${SIMG} dpkg -L bwa samtools bcftools bowtie | grep /bin/)\n\n# Make softlinks pointing to the Singularity image\nfor i in $bins; do echo $i; ln -s ../${SIMG} bin/${i##*/} ; done\n\n# Check what is the content of bin\n[09:11:36]&gt; ls -l bin\ntotal 0\nlrwxrwxrwx 1 user user 15 Apr  4 09:09 ace2sam -&gt; ../samtools.sif\nlrwxrwxrwx 1 user user 15 Apr  4 09:09 bcftools -&gt; ../samtools.sif\nlrwxrwxrwx 1 user user 15 Apr  4 09:09 blast2sam.pl -&gt; ../samtools.sif\nlrwxrwxrwx 1 user user 15 Apr  4 09:09 bowtie -&gt; ../samtools.sif\nlrwxrwxrwx 1 user user 15 Apr  4 09:09 bowtie2sam.pl -&gt; ../samtools.sif\nlrwxrwxrwx 1 user user 15 Apr  4 09:09 bowtie-align-l -&gt; ../samtools.sif\nlrwxrwxrwx 1 user user 15 Apr  4 09:09 bowtie-align-l-debug -&gt; ../samtools.sif\nlrwxrwxrwx 1 user user 15 Apr  4 09:09 bowtie-align-s -&gt; ../samtools.sif\nlrwxrwxrwx 1 user user 15 Apr  4 09:09 bowtie-align-s-debug -&gt; ../samtools.sif\nlrwxrwxrwx 1 user user 15 Apr  4 09:09 bowtie-build -&gt; ../samtools.sif\n...\n</code></pre></p> <p>Let's test the tools (you can add the <code>bin</code> folder in <code>$PATH</code> if you want...) <pre><code>./bin/bwa \n/usr/bin/bwa\n\nProgram: bwa (alignment via Burrows-Wheeler transformation)\nVersion: 0.7.17-r1188\nContact: Heng Li &lt;lh3@sanger.ac.uk&gt;\n\nUsage:   bwa &lt;command&gt; [options]\n\nCommand: index         index sequences in the FASTA format\n         mem           BWA-MEM algorithm\n         fastmap       identify super-maximal exact matches\n         pemerge       merge overlapping paired ends (EXPERIMENTAL)\n         aln           gapped/ungapped alignment\n         samse         generate alignment (single ended)\n         sampe         generate alignment (paired ended)\n         bwasw         BWA-SW for long queries\n...\n</code></pre></p> <pre><code>./bin/samtools --help\n/usr/bin/samtools\n\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.10 (using htslib 1.10.2-3)\n\nUsage:   samtools &lt;command&gt; [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     fqidx          index/extract FASTQ\n     index          index alignment\n...\n</code></pre> <pre><code>./bin/bcftools --help\n/usr/bin/bcftools\n\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nVersion: 1.10.2 (using htslib 1.10.2-3)\n\nUsage:   bcftools [--version|--version-only] [--help] &lt;command&gt; &lt;argument&gt;\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n...\n</code></pre> <p>We can add other tools that we want from the image... <pre><code># Make soft link for date\nln -s ../samtools.sif bin/date\n\n# Running date from the image\n./bin/date\n/usr/bin/date\nMon Apr  4 07:23:16 Europe 2022\n\n# Running date from the container\ndate\nMon 04 Apr 2022 09:24:12 AM CEST\n</code></pre></p> <p>Note the different time zones ;-)</p>"},{"location":"inspect/","title":"Inspecting the metadata of the container","text":"<p>Online documentation</p> <p><pre><code>$ singularity inspect [inspect options...] &lt;image path&gt;\n</code></pre> Inspect will show you labels, environment variables, apps and scripts associated with the image determined by the flags you pass.</p>"},{"location":"inspect/#lets-inspect-our-containers","title":"Let's inspect our containers.","text":"<p><code>-d</code> - show the Singularity recipe file that was used to generate the image</p> <pre><code>$ singularity inspect -d lolcow.sif\n\nBootStrap: docker\nFrom: ubuntu:16.04\n\n%post\n  apt-get -y update\n  apt-get -y install fortune cowsay lolcat\n\n%environment\n  export LC_ALL=C\n  export PATH=/usr/games:$PATH\n\n%runscript\n  fortune | cowsay | lolcat\n</code></pre> <p><code>-e</code> - show the environment settings for the image <pre><code>$ singularity inspect -e lolcow.sif\n\n=== /.singularity.d/env/10-docker2singularity.sh ===\n#!/bin/sh\nexport PATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n\n=== /.singularity.d/env/90-environment.sh ===\n#!/bin/sh\n# Custom environment shell code should follow\n\n\n  export LC_ALL=C\n  export PATH=/usr/games:$PATH\n</code></pre> <code>-r</code> - show the runscript for the image <pre><code>$ singularity inspect -r lolcow.sif\n\n#!/bin/sh\n\n  fortune | cowsay | lolcat\n</code></pre></p> <p>Note</p> <p>Do not assume that the content of the script will give you all the information. Just test with an image you have downloaded from docker://</p> docker:// <p><pre><code>singularity inspect -d wttr.sif\n\nbootstrap: docker\nfrom: dctrud/wttr\n</code></pre> Check the runscript.</p>"},{"location":"installation/","title":"How to install","text":"<p>Information</p> <p>Singularity project is officially moving into the Linux Foundation. As part of this move, and to differentiate from the other like-named projects and commercial products, we will be renaming the project to \"Apptainer\". source</p> <p>Disclaimer</p> <p>During the last 2021 year, the installation instructions changed more frequent than the workshop was given... Always check with the original instructions on how to install Singularity/Apptainer. For the purpose of this workshop, we will try to adapt to the the new free derivative as soon it becomes stable https://github.com/apptainer/apptainer.</p> <p>Until then the content bellow will remain unchanged, to avoid unnecessary modifications.</p> <p>If you have already installed Singularity version, newer than 3.7, it should be sufficient for the workshop.</p>"},{"location":"installation/#singularity-installation-via-package-manager-if-available-linux","title":"Singularity installation via package manager (if available) : Linux","text":"<p>Currently, for supported Ubuntu and CentOS distributions, it is also possible to install Singuarity via the system package manager link <pre><code># Ubuntu 24.04\nwget https://github.com/sylabs/singularity/releases/download/v4.2.2/singularity-ce_4.2.2-noble_amd64.deb\nsudo apt install ./singularity-ce_4.2.2-noble_amd64.deb\n\n# RHEL/CentOS/AlmaLinux/Rocky 9\nwget https://github.com/sylabs/singularity/releases/download/v4.2.2/singularity-ce-4.2.2-1.el9.x86_64.rpm\nsudo yum install ./singularity-ce-4.2.2-1.el9.x86_64.rpm\n</code></pre></p>"},{"location":"installation/#installation-for-windows-or-mac","title":"Installation for Windows or Mac","text":"<p>https://sylabs.io/guides/latest/admin-guide/installation.html#installation-on-windows-or-mac</p> <p>(PM) I have successfully installed Singularity under WSL2, but can't guarantee that it will work in all cases. Look at this page for tips on the typical Windows installation (MacOS is rather similar).</p>"},{"location":"installation/#apptainer-installation","title":"Apptainer installation","text":"<ul> <li>Install from pre-built packages</li> <li>Installation on Windows or Mac</li> </ul> <p>Note</p> <p>There is a possibility to install Apptainer as unprivileged user, providing that the requirements are satisfied.</p>"},{"location":"installation/#testing-the-installation","title":"Testing the installation","text":"<p>https://sylabs.io/guides/latest/admin-guide/installation.html#testing-checking-the-build-configuration</p> <pre><code> singularity \nUsage:\n  singularity [global options...] &lt;command&gt;\n\nAvailable Commands:\n  build       Build a Singularity image\n  cache       Manage the local cache\n  capability  Manage Linux capabilities for users and groups\n  config      Manage various singularity configuration (root user only)\n  delete      Deletes requested image from the library\n  exec        Run a command within a container\n  inspect     Show metadata for an image\n  instance    Manage containers running as services\n  key         Manage OpenPGP keys\n  oci         Manage OCI containers\n  plugin      Manage Singularity plugins\n  pull        Pull an image from a URI\n  push        Upload image to the provided URI\n  remote      Manage singularity remote endpoints, keyservers and OCI/Docker registry credentials\n  run         Run the user-defined default command within a container\n  run-help    Show the user-defined help for an image\n  search      Search a Container Library for images\n  shell       Run a shell within a container\n  sif         siftool is a program for Singularity Image Format (SIF) file manipulation\n  sign        Attach digital signature(s) to an image\n  test        Run the user-defined tests within a container\n  verify      Verify cryptographic signatures attached to an image\n  version     Show the version for Singularity\n\nRun 'singularity --help' for more detailed usage information.\n</code></pre>"},{"location":"installation/#checking-the-configuration","title":"Checking the configuration","text":"<pre><code>singularity buildcfg\nPACKAGE_NAME=singularity\nPACKAGE_VERSION=3.10.2\nBUILDDIR=/root/singularity/builddir\nPREFIX=/usr/local\nEXECPREFIX=/usr/local\nBINDIR=/usr/local/bin\nSBINDIR=/usr/local/sbin\nLIBEXECDIR=/usr/local/libexec\nDATAROOTDIR=/usr/local/share\nDATADIR=/usr/local/share\nSYSCONFDIR=/usr/local/etc\nSHAREDSTATEDIR=/usr/local/com\nLOCALSTATEDIR=/usr/local/var\nRUNSTATEDIR=/usr/local/var/run\nINCLUDEDIR=/usr/local/include\nDOCDIR=/usr/local/share/doc/singularity\nINFODIR=/usr/local/share/info\nLIBDIR=/usr/local/lib\nLOCALEDIR=/usr/local/share/locale\nMANDIR=/usr/local/share/man\nSINGULARITY_CONFDIR=/usr/local/etc/singularity\nSESSIONDIR=/usr/local/var/singularity/mnt/session\nPLUGIN_ROOTDIR=/usr/local/libexec/singularity/plugin\nSINGULARITY_CONF_FILE=/usr/local/etc/singularity/singularity.conf\nSINGULARITY_SUID_INSTALL=1\n</code></pre>"},{"location":"localimage/","title":"Building from Singularity image","text":"<p>Briefly, one can use Singularity image as <code>Bootstrap</code> to build new i.e. to modify the original source image by adding new packages, programs or modifying setting.</p> <pre><code>Bootstrap: localimage\nFrom: /path/to/container/file/or/directory\n</code></pre> <p>Note</p> <p>When building from a local container, all previous definition files that led to the creation of the current container will be stored in a directory within the container called <code>/.singularity.d/bootstrap_history</code>. SingularityCE will also alert you if environment variables have been changed between the base image and the new image during bootstrap.</p> <p>Related documentation online...</p>"},{"location":"mpi_applications/","title":"Singularity and MPI applications","text":"<p>The Singularity documentation is excellent starting point - link  The C3SE Singularity has really nice summary as well - link</p> <p>Here is an example of simple MPI program compiled with OpenMPI 4.1.2 in Ubuntu 22.04 container.</p> <pre><code>Bootstrap:  docker\nFrom: ubuntu:22.04\n\n%setup\n  mkdir -p ${SINGULARITY_ROOTFS}/opt/mpi-test\n\n%files\n  mpi-test.c /opt/mpi-test/\n\n%post\n  apt-get update &amp;&amp; apt-get install -y build-essential mpi-default-dev lsof slurm-client \\\n  &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n  cd /opt/mpi-test &amp;&amp; mpicc -o mpi-test mpi-test.c\n\n%runscript\n  /bin/bash\n</code></pre> mpi-test.c <pre><code>#include &lt;mpi.h&gt;\n#include &lt;stdio.h&gt;\n\nint main(int argc, char** argv) {\n    // Initialize the MPI environment\n    MPI_Init(NULL, NULL);\n\n    // Get the number of processes\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);\n\n    // Get the rank of the process\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);\n\n    // Get the name of the processor\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    int name_len;\n    MPI_Get_processor_name(processor_name, &amp;name_len);\n\n    // Print off a hello world message\n    printf(\"Hello world from processor %s, rank %d out of %d processors\\n\",\n          processor_name, world_rank, world_size);\n\n    // Finalize the MPI environment.\n    MPI_Finalize();\n}\n</code></pre>"},{"location":"mpi_applications/#running-on-single-node","title":"Running on single node","text":"<p>Running MPI on a single machine is almost trivial, since the communication between the processes is done locally. Below are two different scenarios.</p> <p>Host based MPI runtime (note that the host is running the same OpenMPI version) <pre><code>HP-Z2:&gt; mpirun -n 4 singularity exec mpi-test.sif /opt/mpi-test/mpi-test\n\nAuthorization required, but no authorization protocol specified\nAuthorization required, but no authorization protocol specified\nHello world from processor HP-Z2, rank 1 out of 4 processors\nHello world from processor HP-Z2, rank 3 out of 4 processors\nHello world from processor HP-Z2, rank 0 out of 4 processors\nHello world from processor HP-Z2, rank 2 out of 4 processors\n</code></pre></p> <p>Container based MPI runtime <pre><code>HP-Z2:&gt; singularity exec mpi-test.sif mpirun -n 4 /opt/mpi-test/mpi-test\n\nAuthorization required, but no authorization protocol specified\nAuthorization required, but no authorization protocol specified\nHello world from processor HP-Z2, rank 3 out of 4 processors\nHello world from processor HP-Z2, rank 1 out of 4 processors\nHello world from processor HP-Z2, rank 2 out of 4 processors\nHello world from processor HP-Z2, rank 0 out of 4 processors\n</code></pre></p>"},{"location":"mpi_applications/#running-singularity-with-mpi-across-multiple-nodes","title":"Running singularity with MPI across multiple nodes","text":"<p>Running on multiple nodes is a bit of challenge. <code>mpirun</code> needs to know about the allocated resources and your program should be compiled to support the network hardware...</p> <p>Host based MPI runtime (example for running on Rackham@UPPMAX) <pre><code>#!/bin/bash -l\n#SBATCH -A project\n#SBATCH -n 40 -p devel\n#SBATCH -t 5:00\n\nmodule load gcc/11.3.0 openmpi/4.1.2\n\nmpirun singularity exec mpi-test.sif /opt/mpi-test/mpi-test\n</code></pre> Note: <code>openmpi</code> package from the Ubuntu distribution is compiled without <code>SLURM</code> support and the executable has problems to run <code>srun</code> - link</p> output <pre><code>Hello world from processor r484.uppmax.uu.se, rank 8 out of 40 processors\nHello world from processor r484.uppmax.uu.se, rank 2 out of 40 processors\nHello world from processor r484.uppmax.uu.se, rank 3 out of 40 processors\nHello world from processor r484.uppmax.uu.se, rank 4 out of 40 processors\nHello world from processor r484.uppmax.uu.se, rank 9 out of 40 processors\nHello world from processor r484.uppmax.uu.se, rank 0 out of 40 processors\nHello world from processor r484.uppmax.uu.se, rank 1 out of 40 processors\nHello world from processor r484.uppmax.uu.se, rank 5 out of 40 processors\nHello world from processor r484.uppmax.uu.se, rank 6 out of 40 processors\nHello world from processor r484.uppmax.uu.se, rank 7 out of 40 processors\nHello world from processor r484.uppmax.uu.se, rank 10 out of 40 processors\nHello world from processor r484.uppmax.uu.se, rank 11 out of 40 processors\nHello world from processor r484.uppmax.uu.se, rank 12 out of 40 processors\nHello world from processor r484.uppmax.uu.se, rank 13 out of 40 processors\nHello world from processor r484.uppmax.uu.se, rank 14 out of 40 processors\nHello world from processor r484.uppmax.uu.se, rank 15 out of 40 processors\nHello world from processor r484.uppmax.uu.se, rank 16 out of 40 processors\nHello world from processor r484.uppmax.uu.se, rank 17 out of 40 processors\nHello world from processor r484.uppmax.uu.se, rank 18 out of 40 processors\nHello world from processor r484.uppmax.uu.se, rank 19 out of 40 processors\nHello world from processor r485.uppmax.uu.se, rank 20 out of 40 processors\nHello world from processor r485.uppmax.uu.se, rank 23 out of 40 processors\nHello world from processor r485.uppmax.uu.se, rank 25 out of 40 processors\nHello world from processor r485.uppmax.uu.se, rank 26 out of 40 processors\nHello world from processor r485.uppmax.uu.se, rank 29 out of 40 processors\nHello world from processor r485.uppmax.uu.se, rank 33 out of 40 processors\nHello world from processor r485.uppmax.uu.se, rank 24 out of 40 processors\nHello world from processor r485.uppmax.uu.se, rank 27 out of 40 processors\nHello world from processor r485.uppmax.uu.se, rank 28 out of 40 processors\nHello world from processor r485.uppmax.uu.se, rank 31 out of 40 processors\nHello world from processor r485.uppmax.uu.se, rank 32 out of 40 processors\nHello world from processor r485.uppmax.uu.se, rank 37 out of 40 processors\nHello world from processor r485.uppmax.uu.se, rank 39 out of 40 processors\nHello world from processor r485.uppmax.uu.se, rank 21 out of 40 processors\nHello world from processor r485.uppmax.uu.se, rank 22 out of 40 processors\nHello world from processor r485.uppmax.uu.se, rank 30 out of 40 processors\nHello world from processor r485.uppmax.uu.se, rank 35 out of 40 processors\nHello world from processor r485.uppmax.uu.se, rank 36 out of 40 processors\nHello world from processor r485.uppmax.uu.se, rank 38 out of 40 processors\nHello world from processor r485.uppmax.uu.se, rank 34 out of 40 processors\n</code></pre> <p>Container based MPI runtime <pre><code>#!/bin/bash -l\n#SBATCH -A project\n#SBATCH -n 40 -p devel\n#SBATCH -t 5:00\n\nmodule load gcc/11.3.0 openmpi/4.1.2\n\nsingularity exec mpi-test.sif  mpirun --launch-agent 'singularity exec /FULL_PATH/mpi-test.sif orted' /opt/mpi-test/mpi-test\n</code></pre></p> still failing <pre><code>--------------------------------------------------------------------------\nAn ORTE daemon has unexpectedly failed after launch and before\ncommunicating back to mpirun. This could be caused by a number\nof factors, including an inability to create a connection back\nto mpirun due to a lack of common network interfaces and/or no\nroute found between them. Please check network connectivity\n(including firewalls and network routing requirements).\n--------------------------------------------------------------------------\n</code></pre>"},{"location":"mpi_gromacs/","title":"Example - running conteinerized gromacs in parallel","text":"<p>Here is a simple setup to build and test simple Singularity container with Gromacs with binaries for OpenMPI parallelization provided with the Ubuntu 20.04 distribution.</p> <pre><code>Bootstrap: docker\nFrom: ubuntu:20.04\n\n%setup\n\n%files\n\n%environment\n  export LC_ALL=C\n\n%post\n  export LC_ALL=C\n  export DEBIAN_FRONTEND=noninteractive\n\n  apt-get update &amp;&amp; apt-get -y dist-upgrade &amp;&amp; apt-get install -y git wget gawk cmake build-essential libopenmpi-dev openssh-client slurm-client gromacs-openmpi\n\n%runscript\n  /usr/bin/mdrun_mpi \"$@\"\n</code></pre> <p>At the time of writing this page, the recipe will install</p> <ul> <li>GROMACS - mdrun_mpi, 2020.1-Ubuntu-2020.1-1</li> <li>mpirun (Open MPI) 4.0.3</li> </ul> <p>Note</p> <p>Keep in mind that the binaries provided by the package manager are not optimized for the CPU and you could read such message in the output:</p> <p>Compiled SIMD: SSE2, but for this host/run AVX2_256 might be better (see log). The current CPU can measure timings more accurately than the code in mdrun_mpi was configured to use. This might affect your simulation speed as accurate timings are needed for load-balancing. Please consider rebuilding mdrun_mpi with the GMX_USE_RDTSCP=ON CMake option. Reading file benchMEM.tpr, VERSION 4.6.3-dev-20130701-6e3ae9e (single precision) Note: file tpx version 83, software tpx version 119</p> <p>We can use A free GROMACS benchmark set to perform tests with this set.</p>"},{"location":"mpi_gromacs/#running-on-single-node","title":"Running on single node","text":""},{"location":"mpi_gromacs/#image-based-mpi-runtime","title":"Image-based MPI runtime","text":"<pre><code># Run shell in the container\n$ singularity shell Gromacs-openmpi20.sif\n\n# 2 MPI processes, 4 OpenMP threads per MPI process\nSingularity&gt; mpirun -n 2 mdrun_mpi -ntomp 4 -s benchMEM.tpr -nsteps 10000 -resethway\n</code></pre>"},{"location":"mpi_gromacs/#host-based-mpi-runtime","title":"Host-based MPI runtime","text":"<p>On my host which runs Ubuntu 20.04 the OpenMPI version is identical. On other machines you need to make sure to run compatible version. On HPC clusters this should be module which provides OpenMPI compiled with gcc. <pre><code>$ mpirun -n 2 singularity exec Gromacs-openmpi20.sif /usr/bin/mdrun_mpi -ntomp 4 -s benchMEM.tpr -nsteps 10000 -resethway\n\n# or relying on the runscript\n$ mpirun -n 2 Gromacs-openmpi20.sif -ntomp 4 -s benchMEM.tpr -nsteps 10000 -resethway\n</code></pre></p> <p>Note: At present, there is a incompatibility in the OpenMPI version that causes \"Segmentation fault\" when trying to run it on Rackham@UPPMAX. </p> <p>Bootsraping from Ubntu 18.04 will install <code>GROMACS - mdrun_mpi, 2018.1</code> and <code>mpirun (Open MPI) 2.1.1</code> which \"resolves\" the problem and reveals other...</p> <ul> <li> <p>Image-based MPI runtime: Run 2 MPI processes, 20 OpenMP threads per MPI process <pre><code>scontrol show hostname $SLURM_NODELIST &gt; host\nsingularity exec -B /etc/slurm:/etc/slurm-llnl -B /run/munge Gromacs-openmpi18.sif mpirun -n 2 -d -mca plm_base_verbose 10 --launch-agent 'singularity exec Gromacs-openmpi18.sif orted' /usr/bin/mdrun_mpi -ntomp 20 -s benchMEM.tpr -nsteps 10000 -resethway\n</code></pre> Use <code>-d -mca plm_base_verbose 10</code> to get debugging information from <code>mpirun</code>. The slurm client that provides <code>srun</code> in Ubuntu 18.04 is too old to read the current slurm configuration on Rackham@UPPMAX. <pre><code># Ubuntu 18.04 SLURM error\n# ============================\n...\nsrun: error: _parse_next_key: Parsing error at unrecognized key: SlurmctldHost\nsrun: error: Parse error in file /etc/slurm-llnl/slurm.conf line 5: \"SlurmctldHost=rackham-q\"\nsrun: error: _parse_next_key: Parsing error at unrecognized key: CredType\nsrun: error: Parse error in file /etc/slurm-llnl/slurm.conf line 7: \"CredType=cred/munge\"\nsrun: fatal: Unable to process configuration file\n...\n\n# Ubuntu 20.04 SLURM passes but gromacs+mpi binaries fail on Rackham\n# =====================================================================\n...\n[r483.uppmax.uu.se:25362] [[34501,0],0] plm:slurm: final top-level argv:\n        srun --ntasks-per-node=1 --kill-on-bad-exit --nodes=1 --nodelist=r484 --ntasks=1 /usr/bin/singularity exec Gromacs-apt.sif /usr/bin/orted -mca ess \"slurm\" -mca ess\n_base_jobid \"2261057536\" -mca ess_base_vpid \"1\" -mca ess_base_num_procs \"2\" -mca orte_node_regex \"r[3:483-484]@0(2)\" -mca orte_hnp_uri \"2261057536.0;tcp://172.18.10.240,10.1.10.234,10.0.10.234:44203\" -mca plm_base_verbose \"10\" -mca -d \"-\ndisplay-allocation\"\n[r483.uppmax.uu.se:25362] [[34501,0],0] complete_setup on job [34501,1]\n Data for JOB [34501,1] offset 0 Total slots allocated 2\n2261057536.0;tcp://172.18.10.240,10.1.10.234,10.0.10.234:44203\n...\nReading file benchMEM.tpr, VERSION 4.6.3-dev-20130701-6e3ae9e (single precision)\nNote: file tpx version 83, software tpx version 119\n[r484:19455] *** Process received signal ***\n[r484:19455] Signal: Segmentation fault (11)\n...\n</code></pre> Attempting to start the jobs manually requires <code>-mca plm rsh</code> to skip SLURM and use rsh/ssh to start the processes on all allocated nodes and <code>-B /etc/ssh</code> to pick up the \"Host authentication\" setup. Here are the problems:</p> <ul> <li>Rackham@UPPMAX uses \"Host based authentication\" which does not work in the Singularity container, because the container runs in the user space and can not get/read the private host key...</li> <li>One can use passwordless user key for the authentication (Note: passwordless ssh keys are not allowed on Rackham) after adding all nodes public host keys/signatures (the signatures that you are asked to accept when connecting for the first time to a machine).</li> </ul> </li> <li> <p>Host-based MPI runtime: Run 2 MPI processes, 20 OpenMP threads per MPI process (explicitly specified on the mpirun command line) <pre><code>#!/bin/bash -l\n#SBATCH -J test\n#SBATCH -t 00:15:00\n#SBATCH -p devel -N 2 -n 2\n#SBATCH --cpus-per-task 20\n#SBATCH -A project\n\nmodule load gcc/7.2.0 openmpi/2.1.1\n\nmpirun -n 2 Gromacs-openmpi18.sif -ntomp 20 -s benchMEM.tpr -nsteps 10000 -resethway\n# or\n# mpirun -n 2 singularity exec Gromacs-openmpi18.sif /usr/bin/mdrun_mpi -ntomp 20 -s benchMEM.tpr -nsteps 10000 -resethway\n</code></pre></p> </li> </ul>"},{"location":"mpi_gromacs/#compiling-gromacs","title":"Compiling Gromacs","text":"<p>Note: the recipe bellow uses the OpenMPI from the distribution that triggers \"Segmentation fault\" problems when running the container on Rackham@UPPMAX.</p> recipe <pre><code>Bootstrap: docker\nFrom: ubuntu:20.04\n\n%setup\n\n%files\n\n%environment\n  export LC_ALL=C\n\n%post\n  export LC_ALL=C\n  export DEBIAN_FRONTEND=noninteractive\n  export NCPU=$(grep -c ^processor /proc/cpuinfo)\n\n  apt-get update &amp;&amp; apt-get -y dist-upgrade &amp;&amp; apt-get install -y git wget gawk cmake build-essential libopenmpi-dev openssh-client\n\n  mkdir -p installs\n\n  # Gromacs\n  mkdir -p /tmp/downloads &amp;&amp; cd /tmp/downloads\n  test -f gromacs-2021.2.tar.gz || wget https://ftp.gromacs.org/gromacs/gromacs-2021.2.tar.gz\n  tar xf gromacs-2021.2.tar.gz -C /installs\n\n  cd /installs/gromacs-2021.2\n  mkdir build-normal &amp;&amp; cd build-normal\n  cmake .. -DCMAKE_INSTALL_PREFIX=/opt/gromacs-2021.2 -DGMX_GPU=OFF -DGMX_MPI=ON -DGMX_THREAD_MPI=ON -DGMX_BUILD_OWN_FFTW=ON -DGMX_DOUBLE=OFF -DGM\nX_PREFER_STATIC_LIBS=ON -DBUILD_SHARED_LIBS=ON -DCMAKE_BUILD_TYPE=RELEASE\n  make -j $NCPU &amp;&amp; make install\n\n  cd /installs/gromacs-2021.2\n  mkdir build-mdrun-only &amp;&amp; cd build-mdrun-only\n  cmake .. -DCMAKE_INSTALL_PREFIX=/opt/gromacs-2021.2 -DGMX_GPU=OFF -DGMX_MPI=ON -DGMX_THREAD_MPI=ON -DGMX_BUILD_OWN_FFTW=ON -DGMX_DOUBLE=OFF -DGM\nX_PREFER_STATIC_LIBS=ON -DBUILD_SHARED_LIBS=ON -DCMAKE_BUILD_TYPE=RELEASE -DGMX_BUILD_MDRUN_ONLY=ON\n  make -j $NCPU &amp;&amp; make install\n\n  cd /\n  rm -r /installs\n  rm /etc/apt/apt.conf.d/singularity-cache.conf\n\n%runscript\n#!/bin/bash\n  source /opt/gromacs-2021.2/bin/GMXRC\n  exec gmx_mpi \"$@\"\n</code></pre>"},{"location":"mpi_gromacs/#mpi-multi-node-example-hpc2n","title":"MPI multi-node example: HPC2N","text":"<p>This is an example on how to compile GROMACS (but it can be other software too) at HPC2N and how to run it on several nodes  Apptainer multi-node Kebnekaise.</p> <p>GROMACS reminds you: \"Statistics: The only science that enables different experts using the same figures to draw different conclusions.\" (Evan Esar)</p>"},{"location":"multi-stage_builds/","title":"Multi-Stage Builds","text":"<p>Reference to the documentation</p> <p>One might want to install software that requires external libraries that are not available with the distribution or to recompile existing with different options. Usually, this will require installing common building tools and compilers that are not needed for running the executables...</p> <p>Similar to Docker multi-stage builds, Singularity also offers a multi-stage builds that allows for copying files between stages (Singularity can copy only from previous to current stage). Below is an  example definition file that compiles the ARPIP (Ancestral sequence Reconstruction under the Poisson Indel Process) tool. The recipe is following the local installation for the static binary build. </p> <pre><code>Bootstrap: docker\nFrom: ubuntu:20.04\nStage: devel\n\n%post\n  export LC_ALL=C\n  export DEBIAN_FRONTEND=noninteractive\n\n  # Package cache in /tmp\n  mkdir -p /tmp/apt20 &amp;&amp;  echo \"Dir::Cache \"/tmp/apt20\";\" &gt; /etc/apt/apt.conf.d/singularity-cache.conf\n\n  apt-get update &amp;&amp; apt-get -y dist-upgrade &amp;&amp; \\\n  apt-get install -y wget git cmake build-essential zlib1g-dev\n\n  # Download\n  export TMPD=/tmp/downloads &amp;&amp;   mkdir -p $TMPD\n  mkdir -p /installs \n\n  # bpp-core http://biopp.univ-montp2.fr/\n  cd /installs\n  git clone https://github.com/BioPP/bpp-core\n  cd bpp-core\n  git checkout tags/v2.4.1 -b v241\n  mkdir build \n  cd build\n  cmake ..\n  make -j 16 install\n\n  # bpp-seq http://biopp.univ-montp2.fr/\n  cd /installs\n  git clone https://github.com/BioPP/bpp-seq\n  cd bpp-seq\n  git checkout tags/v2.4.1 -b v241\n  mkdir build\n  cd build\n  cmake ..\n  make -j 16 install\n\n  # bpp-phyl http://biopp.univ-montp2.fr/\n  cd /installs\n  git clone https://github.com/BioPP/bpp-phyl\n  cd bpp-phyl\n  git checkout tags/v2.4.1 -b v241\n  mkdir build\n  cd build\n  cmake  ..\n  make -j 16 install\n\n  # boost - C++ Libraries http://www.boost.org/\n  cd /installs\n  wget -P $TMPD -c https://boostorg.jfrog.io/artifactory/main/release/1.79.0/source/boost_1_79_0.tar.gz \n  tar xvf $TMPD/boost_1_79_0.tar.gz\n  cd boost_1_79_0\n  ./bootstrap.sh --prefix=/usr/\n  ./b2 \n  ./b2 install \n\n  # glog - Google Logging Library https://github.com/google/glog/\n  cd /installs\n  git clone -b v0.5.0 https://github.com/google/glog\n  cd glog\n  cmake -H. -Bbuild -G \"Unix Makefiles\"\n  cmake --build build --target install\n\n  # gtest - Google Test Library https://github.com/google/googletest/\n  cd /installs\n  git clone https://github.com/google/googletest.git -b release-1.11.0\n  cd googletest\n  mkdir build\n  cd build\n  cmake ..\n  make -j 4 install\n\n  # ARPIP\n  cd /opt\n  git clone https://github.com/acg-team/bpp-arpip/\n  cd bpp-arpip\n  cmake --target ARPIP -- -DCMAKE_BUILD_TYPE=Release-static CMakeLists.txt\n  make -j 8 \n\n  clean \n  cd / &amp;&amp; rm -rf /installs\n\n#########################################################\n\nBootstrap: docker\nFrom: ubuntu:20.04\nStage: final\n\n%files from devel\n  /opt/bpp-arpip                       /opt/\n  /usr/local/lib/libbpp-core.so.4      /usr/local/lib/libbpp-core.so.4  \n  /usr/local/lib/libbpp-seq.so.12      /usr/local/lib/libbpp-seq.so.12\n  /usr/local/lib/libbpp-phyl.so.12     /usr/local/lib/libbpp-phyl.so.12\n  /usr/local/lib/libglog.so.0          /usr/local/lib/libglog.so.0\n\n%environment\n  export LC_ALL=C\n  export PYTHONNOUSERSITE=True\n\n%post\n  export LC_ALL=C\n  export PYTHONNOUSERSITE=True\n  export DEBIAN_FRONTEND=noninteractive\n\n  # Package cache in /tmp\n  mkdir -p /tmp/apt20 &amp;&amp;  echo \"Dir::Cache \"/tmp/apt20\";\" &gt; /etc/apt/apt.conf.d/singularity-cache.conf\n\n  apt-get update &amp;&amp; apt-get -y dist-upgrade &amp;&amp; \\\n  apt-get install -y wget git libc6 libstdc++6 libgcc-s1 \n\n%runscript\n  /opt/bpp-arpip/ARPIP \"$@\"\n</code></pre> <p><code>Stage: devel</code> lines 1-82 are compiling all the required libraries and tools to compile the ARPIP code. This stage can be used in a container that will run perfectly fine and with a bit more luck, if the final executable was fully static, one can even try to copy the file outside the container and run it as it is. Unfortunately, extracting the executable on Rackham shows these disappointing results. </p> <p>Under Ubuntu 20.04 <code>GLIBC...</code> problems are resolved but <code>libbpp-core.so.4</code>, <code>libbpp-seq.so.12</code>, <code>libbpp-seq.so.12</code>, and <code>libglog.so.0</code> we just compiled remain missing.</p> <p><pre><code>ldd ARPIP \n./ARPIP: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by ./ARPIP)\n./ARPIP: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.26' not found (required by ./ARPIP)\n./ARPIP: /lib64/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by ./ARPIP)\n./ARPIP: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by ./ARPIP)\n./ARPIP: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by ./ARPIP)\n        linux-vdso.so.1 =&gt;  (0x00007ffe9257f000)\n        libbpp-core.so.4 =&gt; not found\n        libbpp-seq.so.12 =&gt; not found\n        libbpp-phyl.so.12 =&gt; not found\n        libglog.so.0 =&gt; not found\n        libpthread.so.0 =&gt; /lib64/libpthread.so.0 (0x00002b529980d000)\n        libstdc++.so.6 =&gt; /lib64/libstdc++.so.6 (0x00002b5299a29000)\n        libm.so.6 =&gt; /lib64/libm.so.6 (0x00002b5299d31000)\n        libgcc_s.so.1 =&gt; /lib64/libgcc_s.so.1 (0x00002b529a033000)\n        libc.so.6 =&gt; /lib64/libc.so.6 (0x00002b529a249000)\n        /lib64/ld-linux-x86-64.so.2 (0x00002b52995e9000)\n</code></pre> And that is what we are doing in <code>Stage: final</code> - we copy the compiled libraries from <code>Stage: devel</code> in to a minimum Ubuntu 20.04 (could be other flavor as well) (lines: 90-95). In this case we avoid \"stuffing\" the container with unnecessary packages needed for compiling - <code>cmake build-essential zlib1g-dev</code>. There are no shortcuts - one needs to check you have everything you need in the new container - in this case <code>apt-get install -y libc6 libstdc++6 libgcc-s1</code> which will make sure we have the remaining libraries in <code>/lib64/...</code>.</p>"},{"location":"openmpi-benchmark/","title":"Singularity MPI (Kebnekaise)","text":""},{"location":"openmpi-benchmark/#setup","title":"Setup","text":"<p>The following definition script (osu_benchmarks.def) was used to generate the Singularity container:</p> <pre><code>Bootstrap: docker\nFrom: ubuntu:20.04\n\n%files\n    /home/pedro/Singularity_MPI/osu-micro-benchmarks-5.8.tgz /root/\n    /home/pedro/Singularity_MPI/openmpi-4.0.3.tar.gz /root/\n\n%environment\n    export OSU_DIR=/usr/local/osu/libexec/osu-micro-benchmarks/mpi\n\n%post\n    apt-get -y update &amp;&amp; DEBIAN_FRONTEND=noninteractive apt-get -y install build-essential gfortran openssh-server\n    cd /root\n    tar zxvf openmpi-4.0.3.tar.gz &amp;&amp; cd openmpi-4.0.3\n    echo \"Configuring and building OpenMPI...\"\n    ./configure --prefix=/usr &amp;&amp; make all install clean\n    export PATH=/usr/bin:$PATH\n    export LD_LIBRARY_PATH=/usr/lib:$LD_LIBRARY_PATH\n    cd /root\n    tar zxvf osu-micro-benchmarks-5.8.tgz\n    cd osu-micro-benchmarks-5.8/\n    echo \"Configuring and building OSU Micro-Benchmarks...\"\n    ./configure --prefix=/usr/local/osu CC=/usr/bin/mpicc CXX=/usr/bin/mpicxx\n    make -j2 &amp;&amp; make install\n\n%runscript\n    echo \"Rank - About to run: ${OSU_DIR}/$*\"\n    exec ${OSU_DIR}/$*\n</code></pre> <p>This is an adaptation of the example from The Carpentries (https://carpentries-incubator.github.io/singularity-introduction/08-singularity-mpi.html). Generation of the container was done through:</p> <pre><code>sudo singularity build osu_benchmarks.sif osu_benchmarks.def \n</code></pre> <p>The container is then transferred to Kebnekaise. Notice that the MPI versions that will be used in Kebnekaise need to match the ones used in the generation of the container. Running the container is done  through the batch system, a typical run for the latency benchmark case is as follows:</p> <pre><code>#!/bin/bash\n#SBATCH -A Project_ID\n#SBATCH -n 2\n#SBATCH -t 00:10:00\n#SBATCH --exclusive\n\nml purge  &gt; /dev/null 2&gt;&amp;1\nml singularity/3.8.2\nml GCC/9.3.0\nml OpenMPI/4.0.3\n\n#MPI compiled\nmpirun -np 2 /osu-micro-benchmarks/mpi/pt2pt/osu_latency\n\n#Singularity container\nmpirun -np 2 singularity run osu_benchmarks.sif pt2pt/osu_latency\n</code></pre>"},{"location":"openmpi-benchmark/#benchmark-results","title":"Benchmark results","text":"<p> Fig.1 - Latency benchmark on Kebnekaise using the on-site MPI-compiled version (black), the singularity container (red) and singularity container compiled with infiniband libraries (orange). There is a 9.6% \"overhead\" in going from MPI-compiled (black) to singularity (red) for 4MB message.</p> <p> Fig.2 - All-to-all benchmark on Kebnekaise using the on-site MPI-compiled version (black), the singularity container (red), singularity container compiled with infiniband libraries (orange), and Apptainer (blue). There is a 24.2% \"overhead\" in going from MPI-compiled (black) to singularity (red) for 1MB message.</p>"},{"location":"own_container/","title":"Building simple container","text":"<p>Let's start with a simple example.  </p> <p>Here is a definition file to install the Paraview program in virtual environment conveniently provided by the Ubuntu distribution via the docker hub repository.</p> <p>paraview.def</p> <pre><code>Bootstrap: docker\nFrom: ubuntu:20.04\n\n%post\n  export DEBIAN_FRONTEND=noninteractive\n\n  apt-get update &amp;&amp; apt-get -y dist-upgrade &amp;&amp; \\\n  apt-get install -y paraview &amp;&amp; \\\n  apt-get clean\n\n  # Patch for old kernels like the one on Rackham running CentOS 7 - 2024.02.23\n  # strip --remove-section=.note.ABI-tag /usr/lib/x86_64-linux-gnu/libQt5Core.so.5\n\n%runscript\n  paraview \"$@\"\n</code></pre> <pre><code>$ sudo singularity build paraview.sif paraview.def\n</code></pre> <p>This will download 301 MB and install 500 new packages... It might take some time to complete, but once you are done you will have a container that will run almost everywhere - there is always a catch.</p> <p>Instead of paraview, modify the definition file to install and run your, not necessarily graphical, program. Few tips: <code>gnuplot</code>, <code>grace</code>, <code>blender</code>, <code>povray</code>, <code>rasmol</code>  ...</p>"},{"location":"practical_information/","title":"Practical information","text":"<p>Information updated: 2025.01.28</p> <p> - Announcement and registration  -  Place: The workshop will be online via Zoom. Link will be sent a week before the workshop.  - Time: - 13 February, 2025 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - 09:15-12:00 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - 13:15-16:00 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0(If you need help with the workshop setup, the meeting will be open from 8:50) \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0(Setting up singularity takes time - please dedicate some time for it in advance)  - Workshop material online </p>"},{"location":"remote_builder/","title":"Building containers remotely","text":"<p>Building Singularity containers might require <code>sudo</code> with <code>root</code> access which usually is not the case on community or public computer resources. Instances running on the cloud are an exception of this general rule.</p> <p>In this course we will limit to exercises with local builds and try building remote when we gather some experience.</p> <p>For now, here are the two most common remote building services (GitHub is experimenting with it as well).</p>"},{"location":"remote_builder/#singularity-container-services","title":"Singularity Container Services","text":"<ul> <li>Default installation of Singularity is configured to connect to the public cloud.sylabs.io services which allows you to send a definition file to be built on the Sylab cloud.</li> <li>Follow the manual about Remote Endpoinst to learn how to build containers remotely.</li> </ul>"},{"location":"remote_builder/#seqera-containers","title":"Seqera containers","text":"<p>Container images are built on-demand by using Seqera\u0092s publicly hosted Wave service. When you request an image, the following steps take place:</p> <ol> <li>The set of packages and their versions are sent to the Seqera Containers API, including configuration settings such as image format (Docker / Singularity) and architecture (amd64 / arm64).</li> <li>The Seqera Containers API validates the request and calls the Wave service API to request the image.</li> <li>The Wave service API returns details such as the image name and build details to the Seqera Containers backend, which are then returned to the web interface.</li> <li>The web interface uses the build ID to query the Wave service API directly for details such as build status and build details.</li> <li>If needed, Wave creates the container build file (either <code>Dockerfile</code> or Singularity recipe) and runs the build. It returns the resulting image and pushes it to the Wave community registry for any subsequent request.</li> </ol>"},{"location":"remote_builder/#building-interactivelly-in-gitpod","title":"Building interactivelly in Gitpod","text":"<p>We are trying to provide an experimental build with graphical interface and Apptainer running in Gitpod. The free tier allows users to run about 50 hours on the standard configuration (4 cores, 8GB RAM, ~30GB storage).</p>"},{"location":"remote_builder/#singularity-container-registry-singularity-hub","title":"Singularity Container Registry (Singularity Hub)","text":"<p>Singularity Hub is the predecessor to Singularity Registry, and while it also serves as an image registry, in addition it provides a cloud build service for users. Singularity Hub also takes advantage of Github for version control of build recipes. The user pushes to Github, a builder is deployed, and the image available to the user. Singularity Hub would allow a user to build and run an image from a resource where he or she doesn't have sudo simply by using Github as a middleman.</p> <p>Notice</p> <p>Singularity Hub is no longer online as a builder service, but exists as a read only archive. Containers built before April 19, 2021 are available at their same pull URLs. To see a last day gallery of Singularity Hub, please see here</p>"},{"location":"remote_repositories/","title":"Running Singularity containers from online repositories","text":"<p>Let us try to run a small and simple container from Docker Hub repository. Singularity, will pull the docker image in the cache, convert it and run it. The output should look something like this:</p> <p>Note</p> <p>Environmental variables that will help you to redirect potentially large folders to alternative location - keep in mind that your <code>$HOME</code> folder is relatively small in size.</p> <pre><code>export PROJECT=project_folder\n\nexport SINGULARITY_CACHEDIR=/proj/${PROJECT}/nobackup/SINGULARITY_CACHEDIR\nexport SINGULARITY_TMPDIR=/proj/${PROJECT}/nobackup/SINGULARITY_TMPDIR\n\nexport APPTAINER_CACHEDIR=/proj/${PROJECT}/nobackup/SINGULARITY_CACHEDIR\nexport APPTAINER_TMPDIR=/proj/${PROJECT}/nobackup/SINGULARITY_TMPDIR\n\nmkdir -p $APPTAINER_CACHEDIR $APPTAINER_TMPDIR\n</code></pre> <pre><code>$ singularity run docker://godlovedc/lolcow\n\nINFO:    Converting OCI blobs to SIF format\nINFO:    Starting build...\nGetting image source signatures\nCopying blob 9fb6c798fa41 done  \nCopying blob 3b61febd4aef done  \nCopying blob 9d99b9777eb0 done  \nCopying blob d010c8cf75d7 done  \nCopying blob 7fac07fb303e done  \nCopying blob 8e860504ff1e done  \nCopying config 73d5b1025f done  \nWriting manifest to image destination\nStoring signatures\n...\n2021/03/15 11:18:19  info unpack layer: sha256:3b61febd4aefe982e0cb9c696d415137384d1a01052b50a85aae46439e15e49a\n2021/03/15 11:18:19  info unpack layer: sha256:9d99b9777eb02b8943c0e72d7a7baec5c782f8fd976825c9d3fb48b3101aacc2\n2021/03/15 11:18:19  info unpack layer: sha256:d010c8cf75d7eb5d2504d5ffa0d19696e8d745a457dd8d28ec6dd41d3763617e\n2021/03/15 11:18:19  info unpack layer: sha256:7fac07fb303e0589b9c23e6f49d5dc1ff9d6f3c8c88cabe768b430bdb47f03a9\n2021/03/15 11:18:19  info unpack layer: sha256:8e860504ff1ee5dc7953672d128ce1e4aa4d8e3716eb39fe710b849c64b20945\nINFO:    Creating SIF file...\n __________________________________\n/ Someone is speaking well of you. \\\n|                                  |\n\\ How unusual!                     /\n ----------------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre> <p>Warning</p> <p>If you experience problem like this  </p> <p>FATAL:   Unable to handle docker://godlovedc/lolcow uri: while building SIF from layers: conveyor failed to get: while checking OCI image: image (linux/amd64) does not satisfy required platform (linux/arm64)</p> <p>use this container instead <code>docker://sylabsio/lolcow</code>. It executes <code>date | cowsay | lolcat</code> in the container instead.</p> <p>The container executes predefined command <code>fortune | cowsay | lolcat</code>.</p> <ul> <li><code>fortune</code> - print a random, hopefully interesting, adage.</li> <li><code>cowsay</code> - configurable speaking/thinking cow (and a bit more)</li> <li><code>lolcat</code> - rainbow coloring effect for text console display</li> </ul> <p>Let's run it again. <pre><code>$ singularity run docker://godlovedc/lolcow\n\nINFO:    Using cached SIF image\n ___________________________\n&lt; You are as I am with You. &gt;\n ---------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre> Note, that singularity, after contacting the repositories, realizes that the container is in the local cache and proceeds to run it. But where is it?</p> <p>More details...</p> <pre><code>$ singularity cache list\n\nThere are 1 container file(s) using 87.96 MiB and 8 oci blob file(s) using 99.09 MiB of space\nTotal space used: 187.04 MiB\n</code></pre> <p>Info</p> <p>Over time the cache will grow and might easily accumulate unnecessary \"blobs\". To clean the cache you can run. <pre><code>$ singularity cache clean\n</code></pre> Here is how the cache might look like: <pre><code>singularity cache clean --dry-run\nUser requested a dry run. Not actually deleting any data!\nINFO:    Removing blob cache entry: blobs\nINFO:    Removing blob cache entry: index.json\nINFO:    Removing blob cache entry: oci-layout\nINFO:    No cached files to remove at /home/ubuntu/.singularity/    cache/library\nINFO:    Removing oci-tmp cache entry:     a692b57abc43035b197b10390ea2c12855d21649f2ea2cc28094d18b93360eeb\nINFO:    No cached files to remove at /home/ubuntu/.singularity/    cache/shub\nINFO:    No cached files to remove at /home/ubuntu/.singularity/    cache/oras\nINFO:    No cached files to remove at /home/ubuntu/.singularity/    cache/net\n</code></pre></p>"},{"location":"remote_repositories/#more-examples","title":"More examples","text":"<p><pre><code>$ singularity run docker://dctrud/wttr\n</code></pre> </p> <p>Info</p> <p> Excellent course material by the CodeRefinery project Containers on HPC with Apptainer</p>"},{"location":"remote_repositories/#metawrap-a-flexible-pipeline-for-genome-resolved-metagenomic-data-analysis","title":"metaWRAP - a flexible pipeline for genome-resolved metagenomic data analysis","text":"<p>Here is an example how to use the metaWRAP pipeline from the docker container - installation instructions.</p> <p><pre><code># Original instructions (do NOT run)\n$ docker pull quay.io/biocontainers/metawrap:1.2--1\n</code></pre> In this particular case it is as easy as:</p> <p><pre><code>$ singularity pull docker://quay.io/biocontainers/metawrap:1.2--1\n\nINFO:    Converting OCI blobs to SIF format\nINFO:    Starting build...\nGetting image source signatures\n...\n</code></pre> This will bring the docker image locally and covert it to Singularity format.</p> <p>Then, one can start the container and use it interactively. In this particular case, executing the Singularity container gives us a shell running in the container.</p> <pre><code>$ ./metawrap_1.2--1.sif\nWARNING: Skipping mount /usr/local/var/singularity/mnt/session/etc/resolv.conf [files]: /etc/resolv.conf doesn't exist in container\n\nSingularity&gt; metawrap --version\nmetaWRAP v=1.2\n</code></pre> <p>To run the tool from the command line (outside of the container, as you would use it in scripts) we need to add the call for the tool.</p> <p>Original commad in the cript: <code>$ metawrap binning -o Lanna-straw_initial_binning_concoct -t 20 -a /proj/test/megahit_ass_Lanna-straw/final.contigs.fa --concoct --run-checkm /proj/test/Lanna-straw_reads_trimmed/*.fastq</code></p> <p>The command now calls the tool from the Singularity container: <code>$ singularity exec metawrap_1.2--1.sif metawrap binning -o Lanna-straw_initial_binning_concoct -t 20 -a /proj/test/megahit_ass_Lanna-straw/final.contigs.fa --concoct --run-checkm /proj/test/Lanna-straw_reads_trimmed/*.fastq</code></p> <p>Pulling Singularity container from online or local library/repository</p> <ul> <li>library:// to build from the Container Library <code>library://sylabs-jms/testing/lolcow</code></li> <li>docker:// to build from Docker Hub <code>docker://godlovedc/lolcow</code></li> <li>shub:// to build from Singularity Hub</li> <li>path to a existing container on your local machine</li> <li>path to a directory to build from a sandbox</li> <li>path to a Singularity definition file</li> </ul>"},{"location":"remote_repositories/#tensorflow","title":"Tensorflow","text":"<p>Let's have some tensorflow running. First <code>pull</code> the image from docker hub (~2.6GB).  </p> <pre><code>$ singularity pull docker://tensorflow/tensorflow:latest-gpu\n\nINFO:    Converting OCI blobs to SIF format\nINFO:    Starting build...\nGetting image source signatures\n...\n</code></pre> <p>If you have a GPU card, here is how easy you can get tensorflow running. Note the <code>--nv</code> option on the command line.</p> <pre><code>$ singularity exec --nv tensorflow_latest-gpu.sif python3\n\nPython 3.8.10 (default, Nov 26 2021, 20:14:08) \n[GCC 9.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import tensorflow as tf\n&gt;&gt;&gt; tf.config.list_physical_devices('GPU')\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n</code></pre>"},{"location":"remote_repositories/#nvidia-deep-learning-frameworks-additional-material","title":"NVIDIA Deep Learning Frameworks - additional material.","text":""},{"location":"reproducibility/","title":"On the subject of reproducibility","text":"<p>There is a good introduction to the problems of reproducibility on the Anaconda blog where 8 levels of reproducibility are defined.</p> <ul> <li>Level -1 (not reproducible)</li> <li>Level 0 (reproducible only by you, today)</li> <li>Level 1 (reproducible by others with guidance)</li> <li>Level 2 (reproducible today, by anyone with Internet access)</li> <li>Level 3 (reproducible indefinitely, by anyone with Internet access)</li> <li>Level 4 (reproducible indefinitely, without depending on the Internet)</li> <li>Level 5 (reproducible with Docker)</li> <li>Level 6 (reproducible with a virtual machine)</li> <li>Level 7 (reproducible on untouched hardware)</li> </ul> <p>Briefly</p> <ul> <li>conda can reach level 3, perhaps level 4 with some archiving.</li> <li>docker is mentioned on level 5, I would say very close to 6 with some platform dependent restrictions...</li> </ul>"},{"location":"reproducibility/#where-singularityapptainer-containers-stand","title":"Where Singularity/Apptainer containers stand","text":"<p>Singularity, in principle, can reach docker's level for particular project that does not require elevated privileges and the same platform dependent restrictions. Essentially, the container build is reproducible as reproducible are the tools used for the build, with the advantage of complete, immutable, and shareable snapshot of the environment setup.  </p> <p>Here is the place to mention a common situation, when Internet is not available (resources with sensitive data and restricted Internet access) and such snapshots (containers, images, etc.) are rather common and easy solution. Quite often, code repositories disappear, change location (<code>pip2</code>) or become obsolete (ex. <code>python2</code>). Having the container, does not solve the reproducibility issue but you are on way better footing with, possibly, still working project or tool.</p>"},{"location":"reproducibility/#danger","title":"Danger !!!","text":"<p>The default Singularity \"mode\" integrates with your data. This is much closer as concept to conda or python environments - you have isolated setup of the tool but you still have access to the other tools on the machine.</p> <p>One of the most common problems is mixing the user's python modules in <code>$HOME/.local/...</code> which by default will be available in the Singularity container and used if matching the python version!!! Simple precaution is to add: <pre><code>%environment\n  export PYTHONNOUSERSITE=True\n</code></pre> which will instruct python to ignore these modules.</p> <p>The problem is identical for <code>R</code>, <code>conda</code>, and other tools that store users modules or libraries in the user's home folder.</p> <p>Docker's default behavior is to run isolated and usually is not affected by this problem. The equivalent to this is to run singularity with <code>--no-home</code> option which will create temporary <code>$HOME</code> directory in memory to keep programs running and you will still have access to the current working directory.</p> <p>Consider, building a container that can run with <code>--contain</code> option as good practices (you do not have to, but in practice the container should be able to run without your specific settings - after all you want to share or run the container on another machine). This is rather restrictive, since it looses all the conveniences, like shell environmental variables, which in tern will disable X11 graphics. For example, to achieve the Docker isolation ,<code>nextflow</code> workflow is running containers like this </p> <p><code>nxf_launch() {set +u; env - PATH=\"$PATH\" ${TMP:+SINGULARITYENV_TMP=\"$TMP\"} ${TMPDIR:+SINGULARITYENV_TMPDIR=\"$TMPDIR\"} ${NXF_TASK_WORKDIR:+SINGULARITYENV_NXF_TASK_WORKDIR=\"$NXF_TASK_WORKDIR\"} singularity exec --no-home --pid --bind /crex/proj/naissXXXX-XX-XX/folder/:/crex/proj/naissXXXX-XX-XX/folder/ /crex/proj/nobackup/NXF_SINGULARITY_CACHEDIR/maestsi-metontiime-latest.img /bin/bash -ue /crex/proj/nobackup/RT-support/Nextflow-test/work/ab/a179275afe402383046320e2da3417/.command.sh}</code></p> <p>Note the <code>--no-home</code> option and the explicit <code>--bind</code>. This is also an excellent approach to test the requirements for your tools. Example: some conda environments setups depend on <code>ffmpeg</code> but mistakenly not listed as users usually have the tool provided by the OS.</p>"},{"location":"reproducibility/#few-hints-that-helps-recreatingrebuilding-your-container","title":"Few hints that helps recreating/rebuilding your container.","text":"<ul> <li>Try to install tools following the best practices for the corresponding tool.</li> <li>Do not be shy to add comments in the definition file.</li> <li>Singularity keeps copy of the definition file <code>singularity inspect -d container.sif</code>. It is rather tedious work to specify all versions of the packages you install with conda, pip, apt, yum, etc. Instead, keep track of the build process <code>sudo singularity build container.sif definition.def |&amp; tee build.log</code> - you or your colleagues might find later this information invaluable to help fix broken rebuild routines.</li> </ul>"},{"location":"reproducibility/#additional-efforts","title":"Additional efforts","text":"<ul> <li>Consider performing test runs during the build and keep the logs for future reference (part of the good practices).</li> <li>Often, licenses restrict code distribution - for your safety, consider keeping a copy of the tool locally as an alternative to the on-line source.</li> <li>As final resort, your containers might be used as source for new build as temporary fix.</li> </ul>"},{"location":"reproducibility/#why-not-talk-more-about-reproducibility-here","title":"Why not talk more about reproducibility here?","text":"<p>The subject is rather complex and problems with reproducibility might extend even beyond software installation.</p> <p>Chemists bitten by Python scripts: How different OSes produced different results during test number-crunching - source</p> <p>Excerpt:</p> <p>When Luo ran these \"Willoughby\u2013Hoye\" scripts, he got different results on different operating systems. For macOS Mavericks and Windows 10, the results were as expected (173.2); for Ubuntu 16 and macOS Mojave, the result differed (172.4 and 172.7 respectively). They may not sound a lot but in the precise world of scientific research, that's a lot.</p> <p>The reason, it turns out, is not specific to Python; rather it's that the underlying system call to read files from a directory leaves the order in which files get read up to the OS's individual implementation. That's why sort order differs in different environments.</p> <p>Here is a bit simplified illustration of the problem from summing the values in 4 files in different order caused by the different localization in the shell.</p> <pre><code>LC_ALL=en_US.UTF-8\na.dat 1.e-8\nA.dat 1.e+8\nb.dat 1.e-8\nB.dat 1.e+8\n200000000.00000002980232238769531250\n\n$LC_ALL=C\nA.dat 1.e+8\nB.dat 1.e+8\na.dat 1.e-8\nb.dat 1.e-8\n200000000.00000000000000000000000000\n</code></pre>"},{"location":"running_singularity/","title":"Running Singularitry container","text":"<p>Let's practice a bit running one of the containers from the previous step. <pre><code>$ singularity pull lolcow.sif docker://godlovedc/lolcow\n</code></pre> This will pull the Docker image in to A singularity container with name <code>lolcow.sif</code>.</p>"},{"location":"running_singularity/#run-the-singularity-container","title":"Run the Singularity container","text":"<p>Executing the container file itself or a remote build will start will execute the commands defined in the <code>%runscript</code> section. This is equivalent to run <code>singularity run ./lolcow.sif</code></p> <pre><code>$ ./lolcow.sif \n_________________________________________\n/ You will stop at nothing to reach your  \\\n| objective, but only because your brakes |\n\\ are defective.                          /\n -----------------------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre> <pre><code>$ singularity run ./lolcow.sif \n _____________________________________\n/ Don't relax! It's only your tension \\\n\\ that's holding you together.        /\n -------------------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre>"},{"location":"running_singularity/#getting-a-shell-in-the-running-container","title":"Getting a shell in the running container","text":"<pre><code>$ singularity shell ./lolcow.sif \nSingularity&gt;\n</code></pre> <p>Let's see what the container gives us. Running the container, runs previously defined command - in this case. <pre><code>/bin/sh -c fortune | cowsay | lolcat\n</code></pre> This uses the container shell, to run three programs that were setup and provided in the container (instead of some heavy calculation tools).</p> <ul> <li>Try to \"play\" with these three tools to get the idea how you ca use them while in the container shell.</li> <li>Can you check on what OS version the container is build? Run <code>cat /etc/os-release</code> \"in\" the container and \"outside\" the container.</li> <li>If you try to list the files in your home folder with <code>ls -l ~</code> you will see the content of your home folder. What about the root folder? List the content with <code>ls -l /</code> and compare the output from a different terminal (outside the container shell). </li> <li>Singularity binds the user home folder, <code>/tmp</code> and some other by default. If you are running on a computer with more users accounts than your own (like on a computer cluster) compare the content of <code>ls -l /home</code> from within the container and outside. You should not be able to see the other users' folders that are on the computer otherwise.</li> </ul>"},{"location":"running_singularity/#execute-program-in-the-container","title":"Execute program in the container","text":"<p>There is of course a way to start a different program than the default shell or the defined in the <code>%runscript</code> section.</p> <pre><code>$ singularity exec ./lolcow.sif fortune\nBeauty and harmony are as necessary to you as the very breath of life.\n\n$ singularity exec  ./lolcow.sif host\nFATAL:   \"host\": executable file not found in $PATH\n</code></pre> <p>Keep in mind that running in the container you should be able to find the program inside or in the folders you have binded to the container i.e. the system tools and programs remain isolated. </p> <p>Warning</p> <p>If you have setup <code>conda</code> or <code>pip</code> installations in you profile folder, they get available in the container. This  might conflict with the container setup which is unaware of programs installed in your home folder. To avoid such situations, you might need to run singularity with <code>--cleanenv</code> option i.e. <code>singularity run -e ./lolcow.sif</code></p>"},{"location":"running_singularity/#bindingmounting-folders","title":"Binding/mounting folders","text":"<p>By default, Singularity binds these folders from the host computer (your computer) to the container defined in <code>/usr/local/etc/singularity/singularity.conf</code>.</p> <pre><code>mount home = yes\n\nmount tmp = yes\n\n#bind path = /etc/singularity/default-nsswitch.conf:/etc/nsswitch.conf\n#bind path = /opt\n#bind path = /scratch\nbind path = /etc/localtime\nbind path = /etc/hosts\n</code></pre> <p>On Rackham few more folders are automatically mounted for almost obvious reasons. <pre><code>- /scratch\n- /sw\n- /proj\n</code></pre></p> <p>Binding folders from the host to the container, allows you to</p> <ul> <li>\"Inject\" any host folder as read/write folder in the container   frequently used for external databases or standalone tools</li> <li>Replace container's folder or file form the host   usually used to provide external configuration files to replace default common.</li> </ul> <p>More detailed online at User-defined bind paths</p>"},{"location":"sandbox/","title":"Building interactively in a sandbox.","text":"<p>Info</p> <p>This section is relevant if you want to build Singularity containers to run on Dardel at PDC, where you might need to run in a <code>sandbox</code> format for performance purposes. More detailed documenation at PDC. Hint: Use <code>ml PDC singularity</code> to load Singularity from the software module system.</p> <p>Unless you know exactly which programs and commands you need to install it might be rather tricky to assemble a recipe that will work. If every change requires rebuilding it becomes rather tedious work. Instead, we can try to build a container as we would do it interactively on the command line.</p> <p>For this purpose, we will use <code>--sandbox</code> option which keeps the file structure intact. A regular build will create this folder structure in the <code>/tmp</code> then at the end will wrap everything in a single compressed read-only file by <code>mksquasfs</code> and delete the folder... The so-called sandbox is writable (by root) and accessible as regular folder (by root).</p> <p>This is extremely convenient for testing purposes. The sandbox folder could be later converted to a regular container - but please do not do it unless you have a good reason why you are breaking all the reproducible features of the container. Instead, during the interactive build, take notes by editing the definition file and build from scratch when you think you are ready.</p> <p>Let's try this with something which might or might not  work - install jupyter with <code>jupyter_contrib_nbextensions</code> and <code>jupyter_nbextensions_configurator</code> via <code>pip</code>.</p> <p>Select location where you will create the new container-folder - in this case jupyter-sb</p> <pre><code>$ sudo singularity build --sandbox jupyter-sb docker://ubuntu:20.04\n\nINFO:    Starting build...\nGetting image source signatures\nCopying blob 5d3b2c2d21bb skipped: already exists  \nCopying blob 3fc2062ea667 skipped: already exists  \nCopying blob 75adf526d75b [--------------------------------------] 0.0b / 0.0b\nCopying config 591d30d91a done  \nWriting manifest to image destination\nStoring signatures\n2021/03/15 13:17:49  info unpack layer: sha256:5d3b2c2d21bba59850dac063bcbb574fddcb6aefb444ffcc63843355d878d54f\n2021/03/15 13:17:51  info unpack layer: sha256:3fc2062ea6672189447be7510fb7d5bc2ef2fda234a04b457d9dda4bba5cc635\n2021/03/15 13:17:51  info unpack layer: sha256:75adf526d75b82eb4f9981cce0b23608ebe6ab85c3e1ab2441f29b302d2f9aa8\nINFO:    Creating sandbox directory...\nINFO:    Build complete: jupyter\n\n# List the current folder - note that the jupyter-sb is owned by root\n$ ls -l\n\ntotal 4\ndrwxr-xr-x 18 root root 4096 Mar 15 13:17 jupyter-sb\n</code></pre> <p>This pulls <code>docker://ubuntu:20.04</code> image and build new singularity container in sandbox in folder <code>jupyter-sb</code> and we add two lines in the new recipe... <pre><code>Bootstrap: docker\nFrom: ubuntu:20.04\n</code></pre></p> <p>Let's \"jump inside\" the container (ignore the warning)</p> <p><pre><code>$ sudo singularity shell --writable jupyter-sb\n\nWARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container\nSingularity&gt;\n</code></pre> Now, you are in the shell inside the container. Keep in mind that you are running as root at this point!</p> <pre><code>$ cat /etc/os-release \n\nNAME=\"Ubuntu\"\nVERSION=\"20.04.2 LTS (Focal Fossa)\"\nID=ubuntu\nID_LIKE=debian\nPRETTY_NAME=\"Ubuntu 20.04.2 LTS\"\nVERSION_ID=\"20.04\"\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nVERSION_CODENAME=focal\nUBUNTU_CODENAME=focal\n</code></pre> <p>To be consistent, run <code>export DEBIAN_FRONTEND=noninteractive</code> (otherwise you will be asked to provide localization interactively during the package installation) and add this line to the <code>%post</code> section of the recipe.</p> <pre><code>Singularity&gt; export DEBIAN_FRONTEND=noninteractive\n\nSingularity&gt; apt-get update\nSingularity&gt; apt-get install -y locales python3-dev  python3-pip python3-tk build-essential bash-completion\n</code></pre> <p>Then the recipe becomes... <pre><code>Bootstrap: docker\nFrom: ubuntu:20.04\n\n%post\n  export DEBIAN_FRONTEND=noninteractive\n\n  apt-get update\n  apt-get install -y locales python3-dev  python3-pip  python3-tk build-essential bash-completion\n</code></pre> We are ready for <code>pip</code></p> <p><pre><code>Singularity&gt; python3 -m pip install --upgrade pip\n\nSingularity&gt; python3 -m pip install jupyter\n\nSingularity&gt; python3 -m pip install jupyter_contrib_nbextensions\nSingularity&gt; jupyter contrib nbextension install --system\nSingularity&gt; jupyter nbextension enable codefolding/main\n\nSingularity&gt; python3 -m pip install jupyter_nbextensions_configurator\nSingularity&gt; jupyter nbextensions_configurator enable --system\n</code></pre> Add the commands in the relevant section. Now, let's try to run the jupyter notebook. </p> <p><pre><code>Singularity&gt; jupyter notebook --ip 0.0.0.0 --no-browser\n\n[I 13:46:05.654 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret\n[I 13:46:05.946 NotebookApp] [jupyter_nbextensions_configurator] enabled 0.4.1\n[C 13:46:05.946 NotebookApp] Running as root is not recommended. Use --allow-root to bypass.\n</code></pre> It complains but it seems that it will work. Exit from the container <code>exit</code>. Add <code>%runscript</code>. Try to build the recipe you have assembled by now.</p> jupyter.def <p><pre><code>Bootstrap: docker\nFrom: ubuntu:20.04\n\n%post\n  export DEBIAN_FRONTEND=noninteractive\n\n  apt-get update\n  apt-get install -y locales python3-dev  python3-pip  python3-tk build-essential bash-completion\n  rm -rf /var/lib/apt/lists/*\n\n  python3 -m pip install --no-cache-dir --upgrade pip\n\n  python3 -m pip install --no-cache-dir jupyter\n\n  python3 -m pip install --no-cache-dir  jupyter_contrib_nbextensions\n  jupyter contrib nbextension install --system\n  jupyter nbextension enable codefolding/main\n\n  python3 -m pip install --no-cache-dir jupyter_nbextensions_configurator\n  jupyter nbextensions_configurator enable --system\n\n%runscript\n  jupyter notebook --ip 0.0.0.0 --no-browser\n</code></pre> <pre><code>$ sudo singularity build jupyter.sif jupyter.def\n</code></pre></p> <p>Note, the added line <code>rm -rf /var/lib/apt/lists/*</code> and options <code>--no-cache-dir</code> to clean or skip remaining cached files.</p> <p>Hint</p> <p>You can convert the sandbox container to a regular single file container with <pre><code>$ singularity build jupyter.sif jupyter-sb\n</code></pre></p> <p>When it is done, try to run it with <code>./jupyter.sif</code>. Does it work? Can you open the address with the browser? Yes, you can install whatever packages you want and they will be available and preset in the container.</p> <p>Keep in mind that you have installed all packages with pip in the container at system level and they will be available to any user running the container. Important! If you or somebody else who will use the container have packages installed in their home folder i.e. <code>pip install --user package</code> they will come on top of everything - for god or bad...</p> <p>When you are done experimenting, and your recipe builds and works, do not forget to delete the sandbox. Be careful, you will need run <code>rm -r</code> with <code>sudo</code>. Slow down. Check twice when you run</p> <pre><code>$ sudo rm -r jupyter-sb\n</code></pre>"},{"location":"seqera-containers/","title":"Seqera containers - brief walk-through","text":"<p>Web page https://seqera.io/containers/</p>"},{"location":"seqera-containers/#using-the-web-interface","title":"Using the web interface","text":"<p>Select the packages you need, select \"Singularity\" and \"linux/amd64\" in the container settings, and push the button to start the building process or get the link for an already available container.</p> <p>Info</p> <p>Container images are built on-demand by using Seqera\u0092s publicly hosted Wave service. When you request an image, the following steps take place:</p> <ol> <li>The set of packages and their versions are sent to the Seqera Containers API, including configuration settings such as image format (Docker / Singularity) and architecture (amd64 / arm64).</li> <li>The Seqera Containers API validates the request and calls the Wave service API to request the image.</li> <li>The Wave service API returns details such as the image name and build details to the Seqera Containers backend, which are then returned to the web interface.</li> <li>The web interface uses the build ID to query the Wave service API directly for details such as build status and build details.</li> <li>If needed, Wave creates the container build file (either <code>Dockerfile</code> or Singularity recipe) and runs the build. It returns the resulting image and pushes it to the Wave community registry for any subsequent request.</li> </ol>"},{"location":"seqera-containers/#command-line-wave-cli","title":"Command line <code>wave-cli</code>","text":"<p>Command line tools GitHub</p> <p>Info</p> <p>Features - Build container images on-demand for a given container file (aka Dockerfile); - Build container images on-demand based on one or more Conda packages; - Build container images for a specified target platform (currently linux/amd64 and linux/arm64); - Push and cache built containers to a user-provided container repository; - Push Singularity native container images to OCI-compliant registries; - Mirror (ie. copy) container images on-demand to a given registry; - Scan container images on-demand for security vulnerabilities;</p>"},{"location":"seqera-containers/#example","title":"Example","text":"<ul> <li>Build and wait til it is done. <pre><code>$ wave -s --freeze --conda-package conda-forge::ase --conda-package conda-forge::xorg-libx11 --await\noras://community.wave.seqera.io/library/ase_xorg-libx11:4f81a2d24b8b708d\n</code></pre></li> <li>Run the tool to test. <pre><code>$  singularity exec oras://community.wave.seqera.io/library/ase_xorg-libx11:4f81a2d24b8b708d ase info \nINFO:    Downloading oras image\n255.0MiB / 255.0MiB [=========================================================================================] 100 % 61.3 MiB/s 0s\n\nplatform                 Linux-6.8.0-45-generic-x86_64-with-glibc2.37\npython-3.13.0            /opt/conda/bin/python3.13\nase-3.23.0               /opt/conda/lib/python3.13/site-packages/ase\nnumpy-2.1.2              /opt/conda/lib/python3.13/site-packages/numpy\nscipy-1.14.1             /opt/conda/lib/python3.13/site-packages/scipy\nmatplotlib-3.9.2         /opt/conda/lib/python3.13/site-packages/matplotlib\nspglib                   not installed\nase_ext                  not installed\nflask-3.0.3              /opt/conda/lib/python3.13/site-packages/flask\npsycopg2                 not installed\npyamg                    not installed\n</code></pre></li> <li>Run the GUI of the tool. <pre><code>$ singularity exec oras://community.wave.seqera.io/library/ase_xorg-libx11:4f81a2d24b8b708d ase gui\nINFO:    Using cached SIF image\n</code></pre> </li> </ul>"},{"location":"ubuntu_template/","title":"Tips collected in an template","text":"<p>Ubuntu definition file</p> <pre><code>Bootstrap: docker\nFrom: ubuntu:24.04\n\n%environment\n  export LC_ALL=C.utf8          \n  export PYTHONNOUSERSITE=True  # Disable user's local python modules in ~/.local\n\n  ## Fix to be able to activate conda environment in the shell\n  ## https://github.com/apptainer/singularity/issues/5075#issuecomment-594391772\n  ## action=\"${0##*/}\"\n  ## if [ \"$action\" = \"shell\" ]; then\n  ##   if [ \"${SINGULARITY_SHELL:-}\" = \"/bin/bash\" ]; then\n  ##     set -- --noprofile --init-file /.singularity_bash\n  ##   elif test -z \"${SINGULARITY_SHELL:-}\"; then\n  ##     export SINGULARITY_SHELL=/bin/bash\n  ##     set -- --noprofile --init-file /.singularity_bash\n  ##   fi\n  ## fi\n\n%post\n  export LC_ALL=C.utf8\n  export PYTHONNOUSERSITE=True\n  export DEBIAN_FRONTEND=noninteractive\n\n  # apt packages cached in /tmp\n  mkdir -p /tmp/apt24 &amp;&amp;  echo \"Dir::Cache \"/tmp/apt24\";\" &gt; /etc/apt/apt.conf.d/singularity-cache.conf\n\n  apt-get update &amp;&amp; apt-get -y dist-upgrade &amp;&amp; \\\n  apt-get install -y git\n\n\n  # Download\n  # export TMPD=/tmp/downloads &amp;&amp;   mkdir -p $TMPD\n  # wget -P $TMPD -c  ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/human_g1k_v37.fasta.gz\n  # gunzip -c $TMPD/human_g1k_v37.fasta.gz &gt; human_g1k_v37.fasta || true\n\n\n  ## Miniforge3\n  ## export TMPD=/tmp/downloads &amp;&amp;   mkdir -p $TMPD\n  ## wget -P $TMPD -c https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh &amp;&amp; \\\n  ## sh $TMPD/Miniforge3-Linux-x86_64.sh -b -p /opt/miniforge3\n  ## echo \"source /opt/miniforge3/etc/profile.d/conda.sh\" &gt;&gt; /.singularity_bash\n  ## echo \"source /opt/miniforge3/etc/profile.d/mamba.sh\" &gt;&gt; /.singularity_bash\n\n\n  # pip cache in /tmp\n  # export PIP_TMP=/tmp/pip-cache ;  mkdir -p $PIP_TMP\n  # python3 -m pip install --cache-dir $PIP_TMP --upgrade pip setuptools wheel\n\n  # pip without caching\n  # python3 -m pip install --no-cache-dir       --upgrade pip setuptools wheel\n\n\n  # Sourcing file when running/executing the container\n  # Note: function declarations are not preserved in the subsequent bash shell\n  # cp /usr/local/gromacs/bin/ACTRC.bash   /.singularity.d/env/99-ACTRC.sh\n\n# cat to a file\n#cat &lt;&lt; EOF &gt; /tmp/myfile\n# ...\n#EOF\n\n# Run in bash sub-shell\n#/bin/bash &lt;&lt;EOF\n# ...\n#EOF\n\n  # Patch for old kernels and Qt5\n  # strip --remove-section=.note.ABI-tag /usr/lib/x86_64-linux-gnu/libQt5Core.so.5 \n\n%runscript\n#!/bin/sh\n  if command -v $SINGULARITY_NAME &gt; /dev/null 2&gt; /dev/null; then\n    exec $SINGULARITY_NAME \"$@\"\n  else\n    echo \"# ERROR !!! Command $SINGULARITY_NAME not found in the container\"\n  fi\n</code></pre>"},{"location":"uppmax_in_a_can/","title":"UPPMAX in a can","text":"<p>https://github.com/UPPMAX/uppmax_in_a_can</p> <p>This Singularity container will let you run a near-identical UPPMAX environment on your own computer. You will have access to all of the installed software at UPPMAX, all your files and reference data on UPPMAX, but it will be your own computer that does the calculations. You can even use it to analyse data that you only have on your own computer, but using the software and reference data on UPPMAX.</p>"},{"location":"uppmax_in_a_can/#typical-use-cases","title":"Typical use cases","text":"<ul> <li>You have sensitive data that is not allowed to leave your computers, but you want to use the programs and references at UPPMAX to do the analysis.</li> <li>You have your own server but want to avoid installing all the software yourself.</li> <li>The queues at UPPMAX are too long or you have run out of core hours but you want the analysis done yesterday.</li> <li>You have your data and compute hours at another SNIC center, but want to use the software installed at UPPMAX.</li> </ul>"},{"location":"uppmax_in_a_can/#what-you-get","title":"What you get","text":"<ul> <li>Access to your home folder and all project folders.</li> <li>Access to all the installed programs at UPPMAX.</li> <li>Access to all reference data at UPPMAX.</li> </ul>"},{"location":"uppmax_in_a_can/#what-you-dont-get","title":"What you don't get","text":"<ul> <li>UPPMAX high-performance computers. You will be limited by the computer you are running the container on.</li> <li>No slurm access. Everything runs on your computer.</li> </ul>"},{"location":"vagrant_windows/","title":"Singularity under Windows","text":""},{"location":"vagrant_windows/#git-bash-virtualbox-vagrant","title":"Git Bash + VirtualBox + Vagrant","text":"<p>Update: 2023.03.22  </p> <p>Just few hints if you follow the official Singularity installation from https://docs.sylabs.io/guides/latest/admin-guide/installation.html#windows</p> <ol> <li>Make sure you start <code>Git Bash</code> terminal.</li> <li><pre><code>$ mkdir vm-singularity-ce &amp;&amp; cd vm-singularity-ce\n$ export VM=sylabs/singularity-ce-3.8-ubuntu-bionic64 &amp;&amp;  vagrant init $VM\n</code></pre></li> <li><code>$ vagrant up</code> - note the last lines that print what folder is shared between the Virtual Machine (VM) and your host computer. <pre><code>$ vagrant up\n...\n    default: shared folder errors, please make sure the guest additions within the\n    default: virtual machine match the version of VirtualBox you have installed on\n    default: your host and reload your VM.\n    default:\n    default: Guest Additions Version: 6.1.22\n    default: VirtualBox Version: 7.0\n==&gt; default: Mounting shared folders...\n    default: /vagrant =&gt; C:/Users/username/vm-singularity\n</code></pre></li> <li><code>$ vagrant ssh</code> will \"ssh\" to the virtual machine that runs Ubuntu 18.04.5 LTS with Singularity 3.8.0 pre-installed. Keep in mind that the VM has only ~1GB RAM, and ~16GB free space. Building large containers might be difficult. You can edit the <code>Vagrantfile</code> file to increase the RAM (look for <code>vb.memory = \"1024\"</code> line and edit the relevant lines).</li> <li>Use the <code>/vagrant</code> folder to copy/transfer files between your host computer and the VM.</li> <li>Do not forget to  <pre><code>$ vagrant destroy &amp;&amp; rm Vagrantfile\n</code></pre> when you finish.</li> </ol>"},{"location":"CaseStudies/ORFfinder/","title":"ORFfinder on Rackham (CentOS7.x)","text":"<p>The tool is distributed precompiled here, but from the release log one can read:</p> <p>02/26/2017 - v 0.4.3 Built with statically linked libstdc++ (on issue of GCC-4.9 libraries not yet supported on CentOS7)</p> <p>It means that the tool will not run on Rackham and Bianca, which currently are running CentOS7.</p> <p>Here is a simple Singularity recipe that will provide newer environment with the necessary libraries...</p> <pre><code>Bootstrap: docker\nFrom: rockylinux:8.5\n\n%environment\n  export LC_ALL=C\n\n%post\n  export LC_ALL=C\n\n  yum update -y &amp;&amp; yum install -y libuv wget gzip libnghttp2 &amp;&amp; yum clean all\n\n  wget -P /tmp -c https://ftp.ncbi.nlm.nih.gov/genomes/TOOLS/ORFfinder/linux-i64/ORFfinder.gz\n  gunzip -k -f /tmp/ORFfinder.gz\n  mv /tmp/ORFfinder /usr/bin/\n  chmod +x /usr/bin/ORFfinder\n\n%runscript\n  /usr/bin/ORFfinder \"$@\"\n</code></pre> <p>Then you can build and name the container to have the same name so you can run it seemingly.</p> <pre><code>$ sudo singularity build ORFfinder Singularity.def\n</code></pre> <pre><code>$ ./ORFfinder -h \nUSAGE\n  ORFfinder [-h] [-help] [-xmlhelp] [-in Input_File] [-id Accession_GI]\n    [-b begin] [-e end] [-c circular] [-g Genetic_code] [-s Start_codon]\n    [-ml minimal_length] [-n nested_ORFs] [-strand Strand] [-out Output_File]\n    [-outfmt output_format] [-logfile File_Name] [-conffile File_Name]\n    [-version] [-version-full] [-version-full-xml] [-version-full-json]\n    [-dryrun]\n\nDESCRIPTION\n   Searching open reading frames in a sequence\n\nUse '-help' to print detailed descriptions of command line arguments\n</code></pre>"},{"location":"CaseStudies/R.ROSETTA/","title":"R.ROSSETTA - Singularity container","text":"<p>Installation instructions https://komorowskilab.github.io/R.ROSETTA/tutorials.html#installation</p> <p>The library requires Wine (compatibility layer capable of running Windows applications) 32 bit...</p> <pre><code>Bootstrap: docker\nFrom: ubuntu:22.04\n\n%files\n  # Add the file in the container \"wine msiexec /i /opt/wine-mono-7.4.0-x86.msi\"\n  # https://dl.winehq.org/wine/wine-mono/7.4.0/wine-mono-7.4.0-x86.msi\n  wine-mono-7.4.0-x86.msi /opt/\n\n\n%environment\n  export LC_ALL=C\n  export PYTHONNOUSERSITE=True\n  export WINEARCH=win32\n\n%post\n  export LC_ALL=C\n  export PYTHONNOUSERSITE=True\n  export DEBIAN_FRONTEND=noninteractive\n\n  # apt packages cached in /tmp\n  mkdir -p /tmp/apt22 &amp;&amp;  echo \"Dir::Cache \"/tmp/apt22\";\" &gt; /etc/apt/apt.conf.d/singularity-cache.conf\n\n  dpkg --add-architecture i386\n  apt-get update &amp;&amp; apt-get -y dist-upgrade \n\n  # R-CRAN\n  apt-get -y  install dirmngr gnupg apt-transport-https ca-certificates software-properties-common\n  apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9\n  add-apt-repository 'deb https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/'\n  apt-get update &amp;&amp; apt-get -y  install r-base\n\n  # Add a default CRAN mirror\n  echo \"options(repos = c(CRAN = 'https://cran.rstudio.com/'), download.file.method = 'libcurl')\" &gt;&gt; /usr/lib/R/etc/Rprofile.site\n\n  # Add a directory for host R libraries\n  mkdir -p /library\n  echo \"R_LIBS_SITE=/library:\\${R_LIBS_SITE}\" &gt;&gt; /usr/lib/R/etc/Renviron.site\n\n  # Install required build dependencies\n  apt-get -y install git wget wine wine32 build-essential \\\n                    libxml2-dev libfontconfig1-dev libcurl4-openssl-dev \\\n                    libssl-dev libharfbuzz-dev libfribidi-dev libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev  \n\n  # Follow th einstructions to install the library\nRscript - &lt;&lt; EOF\n  install.packages(\"devtools\")\n  library(devtools)\n  install_github(\"komorowskilab/R.ROSETTA\")\n  library(R.ROSETTA)\nEOF\n\n\n%runscript\n#!/bin/sh\n  if command -v $SINGULARITY_NAME &gt; /dev/null 2&gt; /dev/null; then\n    exec $SINGULARITY_NAME \"$@\"\n  else\n    echo \"# ERROR !!! Command $SINGULARITY_NAME not found in the container\"\n  fi\n</code></pre> <p>Notes:</p> <ul> <li><code>line 13</code>: Wine needs to run the 32 bit architecture <code>export WINEARCH=win32</code></li> <li><code>line 23</code>: Ubuntu needs to enable 32 bit repositories <code>dpkg --add-architecture i386</code></li> </ul>"},{"location":"CaseStudies/SLURM_in_container/","title":"SLURM capabilities in a container","text":"<p>Case below specific for Rackham Credits to Camille Clouard for the solution, adapted from the original source.</p>"},{"location":"CaseStudies/SLURM_in_container/#bind-the-necessary-tools-and-libraries","title":"Bind the necessary tools and libraries","text":"<p>On Rackham <pre><code>singularity exec -B /usr/bin/sbatch,/usr/lib64/slurm,/etc/slurm,/run/munge,/usr/lib64/libmunge.so.2 container.sif  script.sh\n</code></pre></p> <p>Patch on the fly <code>userID</code> and <code>groupID</code> for the SLURM manager in <code>script.sh</code> before calling SLURM commands.</p> <pre><code>...\n# Params to match SLURMon Rackham   \nexport LD_LIBRARY_PATH=/usr/lib64:$LD_LIBRARY_PATH\necho \"slurm:x:151:151:Slurm:/:/sbin/nologin\" &gt;&gt; /etc/passwd\necho \"slurm:x:151:\" &gt;&gt; /etc/group\n\n/opt/slurm/bin/&lt;slurm_command&gt; &lt;optional_arguments&gt;\n...\n</code></pre>"},{"location":"CaseStudies/VESPA/","title":"VESPA: Very large-scale Evolutionary and Selective Pressure Analyses","text":"<p>Project webpage</p> <p>Here is an example of one very demanding installation with plenty of third party software requirements. Since the environment originally is provided by conda, here we will use <code>From: continuumio/miniconda3</code>.</p> <p>Third party software dependency.</p> Program Version URL BLAST 2.2.30+ ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST DendroPy 4.0 https://pythonhosted.org/DendroPy/#installing MetAL 1.1 http://kumiho.smith.man.ac.uk/blog/whelanlab/?page_id=396 MrBayes 3.2.3 http://mrbayes.sourceforge.net/ MUSCLE 3.8.21 http://www.drive5.com/muscle/downloads.htm NoRMD 1.3 ftp://ftp-igbmc.u-strasbg.fr/pub/NORMD/ PAML 4.4e http://abacus.gene.ucl.ac.uk/software/paml.html ProtTest3 3.4 https://github.com/ddarriba/prottest3 <p>Warning</p> <p>It also requires Python 2.7!</p> <ol> <li>Clone the original repository - there are some binaries that needs to copyed in the container (lines 8 and 9 in the definition file). Download manually the third party software packages. <pre><code>$ git clone https://github.com/aewebb80/VESPA.git\n$ wget -O VESPA.tar.gz https://github.com/aewebb80/VESPA/archive/refs/tags/1.0.1.tar.gz\n$ wget https://github.com/NBISweden/MrBayes/releases/download/v3.2.3/mrbayes-3.2.3.tar.gz\n$ wget https://github.com/ddarriba/prottest3/releases/download/3.4.2-release/prottest-3.4.2-20160508.tar.gz\n$ wget http://www.bork.embl.de/Docu/AQUA/latest/norMD1_3.tar.gz\n</code></pre></li> <li>Here is the Singularity definition file <pre><code>Bootstrap: docker\nFrom: continuumio/miniconda3\n\n%files\n  VESPA.tar.gz /\n  mrbayes-3.2.3.tar.gz /\n  prottest-3.4.2-20160508.tar.gz /\n  VESPA/executables/Linux/normd /usr/local/bin\n  VESPA/executables/Linux/metal /usr/local/bin\n  vespa.yaml /\n\n%environment\n  export LC_ALL=C\n  export PROTTEST_HOME=/opt/prottest-3.4.2\n\n\n%post\n  export LC_ALL=C\n  export PROTTEST_HOME=/opt/prottest-3.4.2\n  export DEBIAN_FRONTEND=noninteractive\n\n  mkdir -p /tmp/apt\n  echo 'Dir::Cache /tmp/apt;'  &gt; /etc/apt/apt.conf.d/singularity-cache.conf\n\n  apt-get update; apt-get -y dist-upgrade &amp;&amp; apt-get install -y wget git ant perl build-essential cmake openjdk-11-jdk autoconf\n\n  # VESPA -----------------------------------------------\n  cd /opt ; tar -xvf /VESPA.tar.gz \n  cd VESPA-1.0.1; chmod +x vespa.py ; cp vespa.py /usr/local/bin\n  chmod +x *Codeml*.pl ; cp *Codeml*.pl /usr/local/bin ; cp -a CodemlWrapper/ /usr/share/perl/5.32\n\n\n  # Beagle ----------------------------------------------\n  cd /opt\n  #export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\n  git clone --depth=1 https://github.com/beagle-dev/beagle-lib.git\n  cd beagle-lib &amp;&amp; mkdir build; cd build \n  cmake ..\n  make install\n\n  # MrBayes ---------------------------------------------\n  cd /opt ; tar -xvf /mrbayes-3.2.3.tar.gz\n  cd mrbayes_3.2.3/src &amp;&amp; autoconf &amp;&amp; ./configure LDFLAGS=\"-Wl,--allow-multiple-definition\" &amp;&amp; make install \n\n  # prottest3 ------------------------------------------\n  cd /opt\n  tar -xvf /prottest-3.4.2-20160508.tar.gz \n\n  # Conda ----------------------------------------------\n  conda update -n base -c defaults conda\n  #conda install -c conda-forge pip mamba\n\n  conda env create -f /vespa.yaml -n VESPA\n  echo \"conda activate VESPA\" &gt;&gt; /opt/conda/etc/profile.d/conda.sh\n\n  conda clean --all --yes\n  rm /*.tar.gz\n\n%runscript\n   params=\"$@\"\n  /bin/bash --rcfile /opt/conda/etc/profile.d/conda.sh -ic \"vespa.py $params\"\n</code></pre> and the adapted <code>vespa.yaml</code> conda enviroment file. <pre><code>name: vespa27\nchannels:\n  - defaults\ndependencies:\n  - conda-forge::python=2.7\n  - conda-forge::numba\n  - conda-forge::h5py\n  - bioconda::blast=2.2.31\n  - bioconda::dendropy=4.2.0\n  - bioconda::muscle=3.8.31\n  - bioconda::paml\n</code></pre></li> </ol> <p>Note, how conda environment <code>VESPA</code> is activated (first line 54, then line 61) to run the tool within the environment.</p>"},{"location":"CaseStudies/VScode/","title":"Run Visual Studio Code with Singularity","text":"<p>https://code.visualstudio.com/</p> <p>This is another real-life scenario. VSCode got rather popular these days, but it is still not available to run on Rackham...</p> <p>Let's try to assemble a recipe and see what difficulties could bring this.</p> <p>For Debian distributions the installation is done via downloading a package .deb file. <code>https://code.visualstudio.com/sha/download?build=stable&amp;os=linux-deb-x64</code></p>"},{"location":"CaseStudies/VScode/#1-the-common","title":"1. The common...","text":"<p>If we choose the \"static\" builds path i.e. not using <code>--sandbox</code>, then we can come up with something like this right away.</p> <pre><code>BootStrap: docker\nFrom: ubuntu:20.04\n\n%post\n  export DEBIAN_FRONTEND=noninteractive\n  apt-get -y update\n  apt-get -y install wget git curl\n\n  cd /\n  wget \"https://code.visualstudio.com/sha/download?build=stable&amp;os=linux-deb-x64\" -O code_stable_amd64.deb\n  apt-get -y install ./code_stable_amd64.deb\n\n  apt-get clean\n\n%environment\n  export LC_ALL=C\n\n%runscript\n  /bin/bash\n</code></pre> <ul> <li>lines 1-6: something trivial. We use docker and Ubuntu.20.04, set <code>export DEBIAN_FRONTEND=noninteractive</code> and update the <code>apt</code> repositories.</li> <li>line 7: make sure we have some tools since VSCode might need them and they are not that big anyway.</li> <li>lines 9-11: downloads the latest stable release and we save it as <code>code_stable_amd64.deb</code>. What happens if you do not specify the name of the output file. Try it in the terminal.</li> <li>line 13: just the usual cleaning.</li> <li>lines 15-16: Some safe defaults</li> <li>lines 18-19: At this point we do not know where the executable will be, so we will start just a bash shell instead.</li> </ul> <p>Build the recipe, and run it. This will start a shell in the container. Use <code>which code</code> to find the location of the VSCode program. <pre><code>$ sudo singularity build vscode.sif Singularity.vscode\n</code></pre> <pre><code>$ ./vscode.sif\nSingularity $ which code\n</code></pre> While in the container, try to start the editor with <code>code</code>.</p> <p>Most probably you will get an error about a missing dynamic library <code>libX11-xcb.so.1</code> (version 1.55.0-1617120720) . Seems that this was not in the dependency of the package...</p>"},{"location":"CaseStudies/VScode/#finding-the-missing-pieces","title":"Finding the missing pieces","text":"<p>This is an easy case - it could be more problematic. Visit https://packages.ubuntu.com/ and use the \"Search the contents of packages\" for <code>focal</code> (Ubuntu 20.04) to find which package could possibly provide the missing library. </p> <p>You should be able to find that <code>libx11-xcb1</code> package contains this file... So, we need to add it to the <code>apt-get install ...</code> line and rebuild. In the general case there might be more missing libraries and using tools like <code>ldd</code> might come more handy to track down multiple missing libraries.</p>"},{"location":"CaseStudies/VScode/#add-the-corrections-and-rebuild","title":"Add the corrections and rebuild","text":"<ul> <li>add <code>libx11-xcb1</code> to line 7</li> <li>replace line 19 with the full path to the program to start <code>/usr/bin/code $@</code></li> </ul> code <pre><code>BootStrap: docker\nFrom: ubuntu:20.04\n\n%post\n  export DEBIAN_FRONTEND=noninteractive\n  apt-get -y update\n  apt-get -y install wget git curl vim  libx11-xcb1 libxshmfence1\n\n  cd /\n  wget \"https://code.visualstudio.com/sha/download?build=stable&amp;os=linux-deb-x64\" -O code_stable_amd64.deb\n  apt-get -y install ./code_stable_amd64.deb\n\n  apt-get clean\n  rm code_stable_amd64.deb\n\n%environment\n  export LC_ALL=C\n\n%runscript\n  /usr/bin/code $@\n</code></pre>"},{"location":"CaseStudies/VScode/#run-again","title":"Run again","text":"<p>Unfortunatelly we are not ready. There will be this pop up window with warnings...</p> <p></p> <p>There might be better ways to do this but at this point we will just give write access to <code>/var</code> to our container.</p> <p><pre><code>$ singularity run -B /run ./vscode.sif \n</code></pre> Note that we did not specify the destination of the folder in the container, but the syntax allows it and this will be equivalent to <code>-B /run:/run</code></p>"},{"location":"CaseStudies/arcasHLA/","title":"arcasHLA in Singularity","text":"<p>Here is the tool's GitHub with rather elaborate dependency list.</p> <p>Note</p> <p>This requires some reasonable efforts to setup. It is doable but if you need to setup this on multiple locations or for multiple users... it becomes unbearable.  </p> <p>The developers of the tool has provided Dockerfile which is perfect guide for installation, by the way. The container is not available to pull from DockerHub so you need to build it yourself and convert it to Sinfularity (relevant info here) or rewrite the recipe for Singularity following the original -  there are tools but it is not that difficult. here it is:</p> <pre><code>Bootstrap: docker\nFrom: ubuntu:18.04\n\n%labels\n  Author pmitev@gmail.com\n\n%environment\n  export LC_ALL=C\n\n%post\n  export DEBIAN_FRONTEND=noninteractive\n  export LANG=C.UTF-8\n  export LC_ALL=C.UTF-8\n\n  export kallisto_version=0.44.0\n  export samtools_version=1.9\n  export bedtools_version=2.29.2\n  export biopython_version=1.77  \n\n  mkdir -p /tmp/apt\n  echo \"Dir::Cache \"/tmp/apt\";\" &gt; /etc/apt/apt.conf.d/singularity-cache.conf\n\n  apt-get update &amp;&amp; \\\n  apt-get  -y --no-install-recommends install \\\n    build-essential \\\n    cmake \\\n    automake \\\n    zlib1g-dev \\\n    libhdf5-dev \\\n    libnss-sss \\\n    curl \\\n    autoconf \\\n    bzip2 \\\n    python3-dev \\\n    python3-pip \\\n    python \\\n    pigz \\\n    git \\\n    libncurses5-dev \\\n    libncursesw5-dev \\\n    libbz2-dev \\\n    liblzma-dev \\\n    bzip2 \\\n    unzip\n\n  python3 -m pip install --upgrade pip setuptools \n  python3 -m pip install --upgrade numpy scipy pandas biopython==${biopython_version}\n\n  # install kallisto\n  mkdir -p /usr/bin/kallisto \\\n    &amp;&amp; curl -SL https://github.com/pachterlab/kallisto/archive/v${kallisto_version}.tar.gz \\\n    | tar -zxvC /usr/bin/kallisto\n\n  mkdir -p /usr/bin/kallisto/kallisto-${kallisto_version}/build\n  cd /usr/bin/kallisto/kallisto-${kallisto_version}/build &amp;&amp; cmake ..\n  cd /usr/bin/kallisto/kallisto-${kallisto_version}/ext/htslib &amp;&amp; autoreconf\n  cd /usr/bin/kallisto/kallisto-${kallisto_version}/build &amp;&amp; make -j4\n  cd /usr/bin/kallisto/kallisto-${kallisto_version}/build &amp;&amp; make install\n\n  # install samtools\n  cd /usr/bin/\n  curl -SL https://github.com/samtools/samtools/releases/download/${samtools_version}/samtools-${samtools_version}.tar.bz2  &gt; samtools-${samtools_version}.tar.bz2\n  tar -xjvf samtools-${samtools_version}.tar.bz2 &amp;&amp;   cd /usr/bin/samtools-${samtools_version} &amp;&amp; ./configure &amp;&amp; make -j4 &amp;&amp; make install\n\n  # install bedtools\n  cd  /usr/bin\n  curl -SL https://github.com/arq5x/bedtools2/releases/download/v${bedtools_version}/bedtools-${bedtools_version}.tar.gz &gt; bedtools-${bedtools_version}.tar.gz\n  tar -xzvf bedtools-${bedtools_version}.tar.gz &amp;&amp; cd /usr/bin/bedtools2 &amp;&amp; make -j4 &amp;&amp; ln -s /usr/bin/bedtools2/bin/bedtools /usr/bin/bedtools\n\n\n  # git lfs\n  curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash\n  apt-get install -y git-lfs \n  git lfs install --system --skip-repo\n\n\n  cd /opt\n  git clone --recursive https://github.com/RabadanLab/arcasHLA.git arcasHLA-master\n\n  rm /etc/apt/apt.conf.d/singularity-cache.conf\n\n%runscript\n  if command -v $SINGULARITY_NAME &gt; /dev/null 2&gt; /dev/null; then\n    exec $SINGULARITY_NAME \"$@\"\n  else\n    echo \"# ERROR !!! Command $SINGULARITY_NAME not found in the container\"\n  fi\n</code></pre> <p>Now, you can easily avoid some unnecessary repetitive installations of multiple tools...</p> <p>With a small trick in the <code>%runscript</code> section, you can make soft links, and you will be able to run other tools from the container - look here for more information.</p>"},{"location":"CaseStudies/arcasHLA/#dependencies","title":"Dependencies:","text":""},{"location":"CaseStudies/arcasHLA/#arcashla-requires-the-following-utilities","title":"arcasHLA requires the following utilities:","text":"<ul> <li>Git Large File Storage</li> <li>coreutils</li> </ul>"},{"location":"CaseStudies/arcasHLA/#make-sure-the-following-programs-are-in-your-path","title":"Make sure the following programs are in your PATH:","text":"<ul> <li>Samtools v1.19</li> <li>bedtools v2.27.1</li> <li>pigz v2.3.1</li> <li>Kallisto v0.44.0</li> <li>Python 3.6</li> </ul>"},{"location":"CaseStudies/arcasHLA/#arcashla-requires-the-following-python-modules","title":"arcasHLA requires the following Python modules:","text":"<ul> <li>Biopython v1.77 (or lower)</li> <li>NumPy</li> <li>SciPy</li> <li>Pandas</li> </ul>"},{"location":"CaseStudies/conda-offline/","title":"Container for offline conda installation","text":"<p>Installing conda packages on a computer without Internet is somewhat challenging. One common approach is to <code>conda-pack</code> your environment, transfer the pack, then \u0093unpack\u0094 it on the other computer - link. Or cache all required packages in a local channel that you can bring offline - link.</p> <p>Here we will demonstrate an easy way to cache the packages in the container, so you can easily install the same environment on a machine without Internet.</p> <p>Here is a simple scenario. </p> <pre><code>$ conda create -y -n test\n$ conda install -y -n test python=3.8 pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch-lts -c nvidia\n</code></pre> <p>Here is a recipe to cache all necessary packages in the container.</p> <pre><code>Bootstrap: docker\nFrom: continuumio/miniconda3\n\n%post\n  export LC_ALL=C\n  conda info\n\n/bin/bash &lt;&lt;EOF\n  conda create -y -n test\n  conda install -y -n test python=3.8 pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch-lts -c nvidia\nEOF\n\n%runscript\n  conda \"$@\"\n</code></pre> <pre><code>$ sudo conda build conda.sif miniconda3.def\n</code></pre> <p>The final container is 6.1 GB ...</p> <p>Then, copy the container on the remote computer without Internet and run exactly the same installation commands <code>--offline</code>. Note the additional <code>--copy</code> option as well.</p> <pre><code>./wharf/conda.sif create -n pytorch-test --offline\n./wharf/conda.sif install -n pytorch-test python=3.8 pytorch torchvision\ntorchaudio cudatoolkit=11.1 -c pytorch-lts -c nvidia --offline --copy\n</code></pre> <p>Here is another variant. You can export your environment  <pre><code>$ conda env export -n my_env &gt; my_env.yaml\n</code></pre></p> <pre><code>Bootstrap: docker\nFrom: continuumio/miniconda3\n\n%files\n  my_env.yaml /opt\n\n%post\n  export LC_ALL=C\n  conda info\n  conda env create -n test -f /opt/my_env.yaml\n\n  conda env remove -n test\n\n%runscript\n  conda \"$@\"\n</code></pre> <p>Then on the target machine: <pre><code>$ ./wharf/conda.sif env create -n my_env -f my_env.yaml --offline --copy\n</code></pre></p>"},{"location":"CaseStudies/fuse-overlayfs/","title":"fuse-overlayfs on Rackham","text":"<p>https://github.com/containers/fuse-overlayfs</p> <p>Here we will skip the explanation on what is and for what purpose you would need <code>fuse-overlayfs</code>, but will rather assume you badly need it to run on a computer or system which does not support it - like Rackham.</p> <p>The recipy is extremely simple - just installing it from the package manager - in this case Ubuntu distribution. <pre><code>Bootstrap: docker\nFrom: ubuntu:20.04\n\n%environment\n  export LC_ALL=C\n\n%post\n  export DEBIAN_FRONTEND=noninteractive\n  export LC_ALL=C\n\n  apt-get update &amp;&amp; apt-get -y dist-upgrade &amp;&amp; apt-get install -y fuse-overlayfs\n\n%runscript\n  /usr/bin/bash \"$@\"\n</code></pre></p> <p>Create the necessary folders for a test setup. <pre><code>$ mkdir -p /tmp/lower /tmp/upper /tmp/workdir /tmp/merged\n$echo \"read-only text\" &gt; /tmp/lower/file.lower\n</code></pre> Here is how it would work if it was available on the system. <pre><code># start the overlayfs \nfuse-overlayfs -o lowerdir=/tmp/lower,upperdir=/tmp/upper,workdir=/tmp/workdir /tmp/merged\n\n# Stop/umount the overlayfs\numount /tmp/merged\n</code></pre> Here is how you would run in with the Singularity container <pre><code>$ singularity shell --fusemount \"container:fuse-overlayfs -o lowerdir=/tmp/lower -o upperdir=/tmp/upper -o workdir=/tmp/workdir /tmp/merged\" fuse-overlayfs.sif\n</code></pre></p>"},{"location":"CaseStudies/gapseq/","title":"Enviroment to run gapseq","text":"<p>Here is a real-life example - you want to run <code>gapseq</code> tool with Singularity. https://gapseq.readthedocs.io/en/latest/install.html </p> <p><code>gapseq</code> is a program for the prediction and analysis of metabolic pathways and genome-scale networks.</p> <p>The tool executable are distributed on GitHub. One can not include it in the container, since the tool downloads data and writes into the code folders...</p> <ul> <li>Create new folder for this project.</li> <li>Use the Ubuntu installation instructions.</li> </ul> <pre><code>sudo apt install ncbi-blast+ git libglpk-dev r-base-core exonerate bedtools barrnap bc\nR -e 'install.packages(c(\"data.table\", \"stringr\", \"sybil\", \"getopt\", \"reshape2\", \"doParallel\", \"foreach\", \"R.utils\", \"stringi\", \"glpkAPI\", \"CHNOSZ\", \"jsonlite\"))'\nR -e 'if (!requireNamespace(\"BiocManager\", quietly = TRUE)) install.packages(\"BiocManager\"); BiocManager::install(\"Biostrings\")'\ngit clone https://github.com/jotech/gapseq &amp;&amp; cd gapseq\n</code></pre> <ul> <li>Do not install the SBML tool (not in the above instructions anyway).</li> <li>Think (discuss) where to clone the GitHUB repository from line 4.   Note that this particular tool downloads external data into the repository structure, which does not work if you include add the repository in the container itself (the common container format is read-only). Thus, the cloning of the repository should be done in your home or project folder where you can run the program with the long syntax i.e.   <pre><code>$ singularity exec ../gapseq.sif ./gapseq doall toy/ecoli.fna.gz\n</code></pre></li> <li>Start the build and save the output to a file to track down potential errors    <pre><code>$ sudo singularity build ... |&amp; tee build.log\n</code></pre></li> <li>Clone the git repository - line 4   <pre><code>$ git clone https://github.com/jotech/gapseq &amp;&amp; cd gapseq\n</code></pre></li> <li>Test the container by running the tool that will start the <code>gapseq</code> tool from the github repository.   <pre><code>$ singularity exec ../gapseq.sif ./gapseq\n</code></pre></li> </ul> output <pre><code>$ singularity exec ../gapseq.sif ./gapseq\n   __ _  __ _ _ __  ___  ___  __ _ \n  / _` |/ _` | '_ \\/ __|/ _ \\/ _` |\n | (_| | (_| | |_) \\__ \\  __/ (_| |\n  \\__, |\\__,_| .__/|___/\\___|\\__, |\n  |___/      |_|                |_|\n\nInformed prediction and analysis of bacterial metabolic pathways     and genome-scale networks\n\nUsage:\n  gapseq test\n  gapseq (find | find-transport | draft | fill | doall | adapt) (..    .)\n  gapseq doall (genome) [medium] [Bacteria|Archaea]\n  gapseq find (-p pathways | -e enzymes) [-b bitscore] (genome)\n  gapseq find-transport [-b bitscore] (genome)\n  gapseq draft (-r reactions | -t transporter -c genome -p     pathways) [-b pos|neg|archaea|auto]\n  gapseq fill (-m draft -n medium -c rxn_weights -g rxn_genes)\n  gapseq adapt (add | remove) (reactions,pathways) (model)\n\nExamples:\n  gapseq test\n  gapseq doall toy/ecoli.fna.gz\n  gapseq doall toy/myb71.fna.gz dat/media/TSBmed.csv\n  gapseq find -p chitin toy/myb71.fna.gz\n  gapseq find -p all toy/myb71.fna.gz\n  gapseq find-transport toy/myb71.fna.gz\n  gapseq draft -r toy/ecoli-all-Reactions.tbl -t toy/    ecoli-Transporter.tbl -c toy/ecoli.fna.gz -p toy/    ecoli-all-Pathways.tbl\n  gapseq fill -m toy/ecoli-draft.RDS -n dat/media/ALLmed.csv -c     toy/ecoli-rxnWeights.RDS -g toy/ecoli-rxnXgenes.RDS\n  gapseq adapt add 14DICHLORBENZDEG-PWY toy/myb71.RDS\n\nOptions:\n  test            Testing dependencies and basic functionality of     gapseq.\n  find            Pathway analysis, try to find enzymes based on     homology.\n  find-transport  Search for transporters based on homology.\n  draft           Draft model construction based on results from     find and find-transport.\n  fill            Gap filling of a model.\n  doall           Combine find, find-transport, draft and fill.\n  adapt           Add or remove reactions or pathways.\n  -v              Show version.\n  -h              Show this screen.\n  -n              Enable noisy verbose mode.\n</code></pre> <ul> <li>Try to run <code>singularity exec ../gapseq.sif ./gapseq test</code>. Did it pass the tests? What is wrong? The output below shows an output with solved R packages tests. The second problem is related to the repository itself.</li> </ul> output <pre><code>$ singularity exec ../gapseq.sif ./gapseq test\ngapseq version: 1.1 7c25ca2\nlinux-gnu\n#74-Ubuntu SMP Wed Jan 27 22:54:38 UTC 2021 \n\n#######################\n#Checking dependencies#\n#######################\nGNU Awk 5.0.1, API: 2.0 (GNU MPFR 4.0.2, GNU MP 6.2.0)\nsed (GNU sed) 4.7\ngrep (GNU grep) 3.4\nThis is perl 5, version 30, subversion 0 (v5.30.0) built for     x86_64-linux-gnu-thread-multi\ntblastn: 2.9.0+\nexonerate from exonerate version 2.4.0\nbedtools v2.27.1\nbarrnap 0.9 - rapid ribosomal RNA prediction\nR version 3.6.3 (2020-02-29) -- \"Holding the Windsock\"\nR scripting front-end version 3.6.3 (2020-02-29)\ngit version 2.25.1\n\nMissing dependencies: 0\n\n\n\n#####################\n#Checking R packages#\n#####################\ndata.table 1.14.0 \nstringr 1.4.0 \nsybil 2.1.5 \ngetopt 1.20.3 \nreshape2 1.4.4 \ndoParallel 1.0.16 \nforeach 1.5.1 \nR.utils 2.10.1 \nstringi 1.5.3 \nglpkAPI 1.3.2 \nBiocManager 1.30.10 \nBiostrings 2.54.0 \njsonlite 1.7.2 \nCHNOSZ 1.4.0 \n\nMissing R packages:  0 \n\n##############################\n#Checking basic functionality#\n##############################\nOptimization test: OK \nCommand line argument error: Argument \"query\". File is not     accessible:  `/opt/gapseq/src/../dat/seq/Bacteria/rev/1.2.4.1.    fasta'\nBlast test: FAILED\n\nPassed tests: 1/2\n</code></pre> <p>Here is a working recipe for the exercise: https://github.com/pmitev/UPPMAX-Singularity/tree/main/gapseq</p>"},{"location":"CaseStudies/github2docker2singularity/","title":"GitHub to DockerHub to Singularity workflow example","text":"<p>Here is an example project on GitHub https://github.com/pwwang/vcfstats that triggers a GutHub Action to build and upload docker container online, that could be easily pulled and used from singularity command line.</p>"},{"location":"CaseStudies/jamovi/","title":"Jamovi: open statistical software for the desktop and cloud","text":"<p>Home page: https://www.jamovi.org/ Git Hub: https://github.com/jamovi/jamovi</p> <p>Installing the software on personal computer is somewhat easy if you have administrative rights. Under Linux, the desktop version is distributed via Flatpak which could be installed with user privileges but the installation does not work over X11, which make it difficult to run remotely on HPC clusters.</p> <p>Fortunately, the cloud version is trivial to run </p> <p>In the <code>docker-compose.yml</code> file one can see that there is an image to pull from DockerHub, so it is worth trying...</p> <pre><code>$ singularity pull docker://jamovi/jamovi\n\nINFO:    Converting OCI blobs to SIF format\nINFO:    Starting build...\nGetting image source signatures\nCopying blob da1fba9c174f done  \nCopying blob ea796d88cf55 done  \nCopying blob 405f018f9d1d done  \nCopying blob bb3005988207 done  \nCopying blob bcb7d4a7ae25 done  \nCopying blob 811da9df0632 done  \nCopying blob 3bc145650b26 done  \nCopying blob 9fe387fb1211 done  \nCopying blob 15ea899e936d done  \nCopying blob 80402b684ebc done  \nCopying blob 159b7fdf9cac done  \nCopying blob b11c49873825 done  \nCopying config dc747a34bd done  \nWriting manifest to image destination\nStoring signatures\n2023/02/03 11:46:53  info unpack layer: sha256:405f018f9d1d0f351c196b841a7c7f226fb8ea448acd6339a9ed8741600275a2\n2023/02/03 11:46:54  info unpack layer: sha256:ea796d88cf556b1104118dc27d524d8bd8dce5886f4f4b1b4dc7452ecdcc3b73\n2023/02/03 11:46:54  info unpack layer: sha256:811da9df0632cdb24c1814f2179fb3e0a9635b059f9631ae4f25b91626509c28\n2023/02/03 11:46:54  info unpack layer: sha256:bcb7d4a7ae25cdd8636c05d7919f2d58aec82dba4d39b0f61ce1e7365a3d05ea\n2023/02/03 11:46:55  info unpack layer: sha256:bb30059882075224c48f7bc3ec67899decd88fe94c4211fdbe39b7208ee6bc63\n2023/02/03 11:46:56  info unpack layer: sha256:da1fba9c174fe439a322fe215815c0c910b68133c3f9ae5e5f60410f8588c35a\n2023/02/03 11:46:59  info unpack layer: sha256:3bc145650b262ff45b2853646ab46230d45c0ac2a5202eda32f5bcde3aba694a\n2023/02/03 11:46:59  info unpack layer: sha256:9fe387fb1211e4512309c4bd4f002d0061a880d00ce1bc176940c29755abebf7\n2023/02/03 11:47:00  info unpack layer: sha256:15ea899e936dab6bbbb4473e4e6ba5e270251bb5e4dee9c53786acb65ec8ba5e\n2023/02/03 11:47:00  info unpack layer: sha256:80402b684ebcc0299fbeb5d43033a39713d5b8136f28a3b99b2c0526d9084bf0\n2023/02/03 11:47:00  info unpack layer: sha256:159b7fdf9cac9720b3aeae8b55ed23d7efc308b642b3ba0b9ffa696cba882e5e\n2023/02/03 11:47:00  info unpack layer: sha256:b11c49873825643bedfd1cad8bf88f4c4003695c8ec9ed3fc47200ce60567df2\nINFO:    Creating SIF file...\n</code></pre> <p>Let's try to run it.</p> <p><pre><code>$ ./jamovi_latest.sif \n/usr/lib/jamovi/server/jamovi/server/__main__.py:116: DeprecationWarning: There is no current event loop\n  loop = get_event_loop()\njamovi\nversion: 0.0.0\ncli:     0.0.0\njamovi.server.server - listening across origin(s): 127.0.0.1:44431 127.0.0.1:39599 127.0.0.1:34611\njamovi.server.server - jamovi accessible from: 127.0.0.1:44431/?access_key=4fee88c51c854faa9d1b5d2486e6791a\nports: 44431, 39599, 34611, access_key: 4fee88c51c854faa9d1b5d2486e6791a\n</code></pre> And open it in a browser...</p> <p></p> <p>Rather good results for the efforts...</p> <p>If you want to build the very latest version, you need to build the docker image yourself and then build the Singularity container from your local source - look here for details.</p>"},{"location":"CaseStudies/mariadb/","title":"Running MariaDB server with Singularity","text":"<p>Note</p> <p>Before we even start, there is a better way to do this nowadays with this excellent and specially designed tool DBdeployer.</p> <p>Anyway, here is how this could be done with singularity.</p> <p>Original source: https://github.com/sylabs/examples/tree/master/database/mariadb</p> <ol> <li>Extract/copy <code>/etc/mysql/my.cnf</code> and edit the user, so it could run as specified user.</li> <li>Build the Singularity container. <pre><code>Bootstrap: docker\nFrom: mariadb:10.3.9\n\n# https://github.com/sylabs/examples/tree/master/database/mariadb\n\n%post\n  export DEBIAN_FRONTEND=noninteractive\n  export LC_ALL=C\n\n  apt update &amp;&amp; apt -y install vim \n  # replace `your-name` with your username, run `whoami` to see your username\n  #YOUR_USERNAME=\"your-name\"\n\n  #sed -ie \"s/^#user.*/user = ${YOUR_USERNAME}/\" /etc/mysql/my.cnf\n  #chmod 1777 /run/mysqld\n\n%runscript\nexec \"mysqld\" \"$@\"\n\n%startscript\nexec \"mysqld_safe\"\n</code></pre></li> <li>Start the server <pre><code>$ singularity shell --writable-tmpfs -B db/:/var/lib/mysql  -B my.cnf:/etc/mysql/my.cnf mariadb.sif\n</code></pre> Alternatively, you can make the configuration file writable by using overlays. <pre><code># add overlay\nsingularity overlay create --size 64 --create-dir /etc/mysql mariadb.sif\n\n# start shell\nsingularity shell --writable-tmpfs -B db/:/var/lib/mysql mariadb.sif\n# Edit the user inplace\nsed -ie \"s/^#user.*/user = NEWUSER/\" /etc/mysql/my.cnf\n</code></pre></li> </ol>"},{"location":"CaseStudies/other_projects/","title":"Other projects on the net","text":""},{"location":"CaseStudies/other_projects/#biocontainers","title":"BioContainers","text":"<p>BioContainers is a community-driven project that provides the infrastructure and basic guidelines to create, manage and distribute bioinformatics packages (e.g conda) and containers (e.g docker, singularity). BioContainers is based on the popular frameworks Conda, Docker and Singularity. Quick links: - https://biocontainers.pro/registry - https://github.com/BioContainers/containers</p>"},{"location":"CaseStudies/other_projects/#ngc-catalog","title":"NGC Catalog","text":"<p>The NGC catalog hosts containers for AI/ML, metaverse, and HPC applications and are performance-optimized, tested, and ready to deploy on GPU-powered on-prem, cloud, and edge systems.</p>"},{"location":"CaseStudies/other_projects/#seqera-containers","title":"Seqera containers","text":"<p>Container images are built on-demand by using Seqera\u0092s publicly hosted Wave service. When you request an image, the following steps take place:</p> <ol> <li>The set of packages and their versions are sent to the Seqera Containers API, including configuration settings such as image format (Docker / Singularity) and architecture (amd64 / arm64).</li> <li>The Seqera Containers API validates the request and calls the Wave service API to request the image.</li> <li>The Wave service API returns details such as the image name and build details to the Seqera Containers backend, which are then returned to the web interface.</li> <li>The web interface uses the build ID to query the Wave service API directly for details such as build status and build details.</li> <li>If needed, Wave creates the container build file (either <code>Dockerfile</code> or Singularity recipe) and runs the build. It returns the resulting image and pushes it to the Wave community registry for any subsequent request.</li> </ol>"},{"location":"CaseStudies/other_projects/#container-recipes-provided-at-c3se-clusters","title":"Container recipes provided at C3SE clusters","text":"<p>This repository holds the recipes of the centrally provided Singularity containers at C3SE's clusters. They are also useable as a reference for users who wish to build their own Singularity containers.</p>"},{"location":"CaseStudies/other_projects/#imageblueprint","title":"ImageBlueprint","text":"<p>Apptainer image tailored for bioinformatics workflows, focusing on Python and R packages, and configuring the image to run Jupyter Notebooks by default.</p>"},{"location":"CaseStudies/portable_biotools/","title":"Portable BioTools","text":"<p>This setup works on any computer with installed Singularity/Apptainer.</p>"},{"location":"CaseStudies/portable_biotools/#setup-details-20231101","title":"Setup details (2023.11.01)","text":"<p>https://github.com/pmitev/UPPMAX-Singularity/tree/main/BioTools</p> <p>Avalable on</p> <ul> <li>Rackham/Bianca: <code>/sw/apps/pm-tools/latest/rackham/singularity/BioTools/bin</code></li> <li>Dardel: <code>/pdc/software/uppmax_legacy/pm-tools/singularity/BioTools/bin</code></li> </ul>"},{"location":"CaseStudies/portable_biotools/#tools-in-the-container","title":"Tools in the container","text":"<p>Note: some tool names are different in the container</p> Name Package blast+ ncbi-blast+ deeptools python3-deeptools <pre><code>abyss               2.3.5+dfsg-2\naugustus            3.5.0+dfsg-2\nbbmap               39.01+dfsg-2\nbcftools            1.16-1\nbeast2-mcmc         2.7.3+dfsg-1\nbedtools            2.30.0+dfsg-3\nbioperl             1.7.8-1\nbowtie2             2.5.0-3+b2\nbusco               5.4.4-1\nbwa                 0.7.17-7+b2\ncanu                2.0+dfsg-2+b1\ncd-hit              4.8.1-4\ncutadapt            4.2-1\ndelly               1.1.6-1\nexonerate           2.4.0-5\nfasta3              36.3.8i.14-Nov-2020-1\nfastp               0.23.2+dfsg-2+b1\nfastqc              0.11.9+dfsg-6\ngffread             0.12.7-3\nhmmer               3.3.2+dfsg-1\nigv                 2.16.0+dfsg-1\ninfernal            1.1.4-1\njellyfish           2.3.0-15+b3\nkisto               0.48.0+dfsg-3\nkraken2             2.1.2-2\nkraken              1.1.1-4\nmacs                2.2.7.1-6+b1\nmafft               7.505-1\nminimap2            2.24+dfsg-3+b1\nmrbayes             3.2.7a-6\nmultiqc             1.14+dfsg-1\nncbi-blast+         2.12.0+ds-3+b1\npaml                4.9j+dfsg-4\npbbamtools          2.1.0+dfsg-2\nphast               1.6+dfsg-3+b1\npicard              2.8.5-1+b1\npilon               1.24-2\npython3-deeptools   3.5.1-3\nqcumber             2.3.0-2\nqiime               2022.11.1-2\nquicktree           2.5-5\nray                 2.3.1-7\nsalmon              1.10.1+ds1-1+b1\nsamblaster          0.1.26-4\nsamclip             0.4.0-4\nsamtools            1.16.1-1\nsnap-aligner        2.0.2+dfsg-1\nsnpeff              5.1+d+dfsg-3\nspades              3.15.5+dfsg-2\nspaln               2.4.13f+dfsg-1\nstacks              2.62+dfsg-1\nSTAR                2.7.11a\nstringtie           2.2.1+ds-2\ntophat-recondition  1.4-3\ntrf                 4.09.1-6\ntrim-galore         0.6.10-1\nvelvet              1.2.10+dfsg1-8\nvmatch              2.3.1+dfsg-8\nvsearch             2.22.1-1\nwham-align          0.1.5-8\n</code></pre>"},{"location":"CaseStudies/portable_biotools/#setup-on-new-location","title":"Setup on new location","text":"<ol> <li>Bring the <code>BioTools-ubuntu.sif</code> container to your project folder.</li> <li>Create links for the installed tools in sub-folder <code>bin</code>.     <pre><code># Navigate to the folder where the bin folder with tools will be created\n# Avoid soft links in the folder location!!!\ncd your_folder\n\n# Run the script\nsingularity exec BioTools-debian.sif /opt/make_links.sh\n</code></pre></li> <li>Add the path to your environment $PATH     <pre><code>export PATH=$PATH:your_folder/bin\n</code></pre></li> <li>Check the tool versions in the container     <pre><code>singularity exec BioTools-debian.sif /opt/package-versions.sh\n</code></pre></li> </ol>"},{"location":"CaseStudies/portable_biotools/#test-running","title":"Test / running","text":"<p><pre><code>$ blastp -version\nblastp: 2.12.0+\n Package: blast 2.12.0, build Mar  8 2022 16:19:08\n\n$ bwa \nProgram: bwa (alignment via Burrows-Wheeler transformation)\nVersion: 0.7.17-r1188\n\n$ bcftools --version-only\n1.13+htslib-1.13+ds\n\n$ busco -v \nBUSCO 5.2.2\n\n$ STAR --version\n2.7.10a\n\n$ igv\n...\n</code></pre> </p>"},{"location":"CaseStudies/qemu-utils/","title":"QEMU-utils","text":"<p>This is just another illustration for using tools in Singularity container.</p> <p>Suppose, you want to upload an VM OS image to the cloud - in this example SNIC Science Cloud (SSC). Ubuntu provides them here and the format available is <code>.img</code>. For one or another reason (under heavy load compressed formats time out), you need to convert it to <code>.raw</code> with:</p> <p><pre><code>$ qemu-img convert -p -f qcow2 -O raw focal-server-cloudimg-amd64.img  focal-server-cloudimg-amd64.raw\n</code></pre> Now, you can certainly do this on your computer, but you will end up with ~ 640MB <code>.img</code> to download and  ~2GB <code>.raw</code> image that you need to upload. Depending on the Internet speed you have, this might be rather slow process.</p> <p>If you have an account on UPPMAX and want to upload your image to the EAST region hosted by UPPMAX... yes, it is rather fast to do this operations remotely on Rackham. The problem is that <code>qemu-tools</code> are not available on Rackham...</p> <p><pre><code>Bootstrap: docker\nFrom: ubuntu:latest\n\n%environment\n  export LC_ALL=C\n\n%post\n  export DEBIAN_FRONTEND=noninteractive\n  export LC_ALL=C\n\n  apt-get update &amp;&amp; apt-get -y dist-upgrade &amp;&amp; apt-get install -y git wget qemu-utils\n\n%runscript\n  /usr/bin/bash \"$@\"\n</code></pre> Here is a bash script that will do everything... <pre><code>#!/bin/bash\nCMD_qemu_cmd=\"singularity exec /crex/proj/nobackup/sbin/qemu-utils.sif qemu-img\"\n\nTMP_DIR=$(mktemp -d) &amp;&amp; \\\ncd ${TMP_DIR} &amp;&amp; echo \"TMP_DIR: \"${TMP_DIR} &amp;&amp; \\\nwget https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img &amp;&amp; \\\n${CMD_qemu_cmd} convert -p -f qcow2 -O raw focal-server-cloudimg-amd64.img  focal-server-cloudimg-amd64.raw &amp;&amp; \\\nIMAGE_NAME=$(date -r focal-server-cloudimg-amd64.img +\"Ubuntu 20.04 - %Y.%m.%d\") &amp;&amp; \\\nopenstack image create --min-disk 20 --private --file focal-server-cloudimg-amd64.raw \"${IMAGE_NAME}\" &amp;&amp; \\\ncd -\n</code></pre></p> <p>Note</p> <p>You need to have the <code>python-openstackclient</code> to be able to run the command <code>openstack image create ...</code></p>"},{"location":"CaseStudies/r-rocker-project/","title":"Rocker project","text":"<p>Running Rstudio server in a Singularityi/Apptainer container</p> <p>https://rocker-project.org/use/singularity.html</p> <p>The project builds and shared docker containers but with support to covert and run it with Singularity/Apptainer. Look for more details on the project web page.</p> <pre><code># Pull the container locally for convinience\nsingularity pull docker://rocker/rstudio:4.4.2\n\n# Run the container\nsingularity exec \\\n   --scratch /run,/var/lib/rstudio-server \\\n   --workdir $(mktemp -d) \\\n   rstudio_4.4.2.sif \\\n   rserver --www-address=127.0.0.1 --server-user=$(whoami)\n</code></pre> <p>This will run rserver in a Singularity container. The <code>--www-address=127.0.0.1</code> option binds to localhost (the default is <code>0.0.0.0</code>, or all IP addresses on the host). listening on <code>127.0.0.1:8787</code>.</p> <p>Note</p> <p>If you run on a remote machine, like computer center, the most efficient way to open the remte server is to use ssh port forwarding, for example  <pre><code>ssh -L 8787:localhost:8787  userID@remote.computer.se\n</code></pre> Quite often, you will run the R studio server in an interactive session/reserved compute node. Then you need to redirect to that node. <pre><code>ssh -L 8787:comp_node_xx:8787  userID@remote.computer.se\n</code></pre> Finally, it might happen that another user is using the same <code>8787</code> port. Take a random number above 1024 and add this option <code>--www-port=9090</code> at the end of the  <code>singularity exec ...</code> line, where <code>9090</code> is the port we have chosen in this example.</p> <p>Look for more details on the web page on how to protect the server with password or submit a SLURM job.</p>"}]}